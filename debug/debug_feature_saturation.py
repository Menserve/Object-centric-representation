"""
ç‰¹å¾´é‡ã®é£½å’Œãƒã‚§ãƒƒã‚¯ï¼šDINOv2 â†’ 2å±¤MLPå¾Œã®ç‰¹å¾´é‡ã®æ¨™æº–åå·®ã‚’ç¢ºèª
"""
import torch
import sys
sys.path.insert(0, '.')
from savi_dinosaur import SAViDinosaur
from train_movi import MoviDataset
import numpy as np

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
checkpoint_path = '../checkpoints/twolayer_mlp_200ep/dinov2_vits14/best_model.pt'
checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

model = SAViDinosaur(num_slots=5, backbone='dinov2_vits14', slot_dim=64)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
dataset = MoviDataset('../data/movi_a_subset', split='all', max_frames=1)
sample = dataset[0]
video = sample['video'].unsqueeze(0)  # (1, T, 3, H, W)

with torch.no_grad():
    # DINOv2ç‰¹å¾´é‡
    dino_features = model.feature_extractor(video[:, 0])  # (1, 384, 16, 16)
    print("ðŸ” DINOv2 features:")
    print(f"  Shape: {dino_features.shape}")
    print(f"  Mean: {dino_features.mean().item():.6f}")
    print(f"  Std: {dino_features.std().item():.6f}")
    print(f"  Min: {dino_features.min().item():.6f}")
    print(f"  Max: {dino_features.max().item():.6f}")
    
    # Reshape to (B, N, D)
    b, c, h, w = dino_features.shape
    features_perm = dino_features.permute(0, 2, 3, 1)  # (1, 16, 16, 384)
    features_flat = features_perm.reshape(b, -1, c)  # (1, 256, 384)
    
    print("\nðŸ” Flattened features (before projection):")
    print(f"  Shape: {features_flat.shape}")
    print(f"  Mean: {features_flat.mean().item():.6f}")
    print(f"  Std: {features_flat.std().item():.6f}")
    
    # 2å±¤MLPæŠ•å½±å¾Œ
    features_projected = model.feature_projection(features_flat)  # (1, 256, 64)
    
    print("\nðŸ” Projected features (after 2-layer MLP):")
    print(f"  Shape: {features_projected.shape}")
    print(f"  Mean: {features_projected.mean().item():.6f}")
    print(f"  Std: {features_projected.std().item():.6f}")
    print(f"  Min: {features_projected.min().item():.6f}")
    print(f"  Max: {features_projected.max().item():.6f}")
    
    # å„ãƒ‘ãƒƒãƒãƒ™ã‚¯ãƒˆãƒ«ã®æ¨™æº–åå·®ï¼ˆãƒ‘ãƒƒãƒé–“ã®å¤šæ§˜æ€§ï¼‰
    patch_stds = features_projected[0].std(dim=1)  # (256,) - å„ãƒ‘ãƒƒãƒã®64æ¬¡å…ƒç‰¹å¾´ã®æ¨™æº–åå·®
    print(f"\nðŸ“Š Per-patch std (diversity within each patch):")
    print(f"  Mean: {patch_stds.mean().item():.6f}")
    print(f"  Std: {patch_stds.std().item():.6f}")
    print(f"  Min: {patch_stds.min().item():.6f}")
    print(f"  Max: {patch_stds.max().item():.6f}")
    
    # ãƒ‘ãƒƒãƒé–“ã®ã‚³ã‚µã‚¤ãƒ³é¡žä¼¼åº¦ï¼ˆå…¨ãƒ‘ãƒƒãƒãŒåŒã˜ãƒ™ã‚¯ãƒˆãƒ«ã«ãªã£ã¦ã„ãªã„ã‹ï¼‰
    features_norm = torch.nn.functional.normalize(features_projected[0], dim=1)  # (256, 64)
    similarity_matrix = torch.mm(features_norm, features_norm.t())  # (256, 256)
    
    # å¯¾è§’ä»¥å¤–ã®é¡žä¼¼åº¦
    mask = torch.eye(256) == 0
    off_diag_similarities = similarity_matrix[mask]
    
    print(f"\nðŸ“Š Patch-to-patch cosine similarity (off-diagonal):")
    print(f"  Mean: {off_diag_similarities.mean().item():.6f}")
    print(f"  Std: {off_diag_similarities.std().item():.6f}")
    print(f"  Min: {off_diag_similarities.min().item():.6f}")
    print(f"  Max: {off_diag_similarities.max().item():.6f}")
    
    if off_diag_similarities.mean().item() > 0.95:
        print("\nâŒ PROBLEM: All patches are nearly identical (similarity > 0.95)")
        print("   The 2-layer MLP is collapsing features into a single point!")
    elif off_diag_similarities.mean().item() > 0.8:
        print("\nâš ï¸  WARNING: Patches are very similar (similarity > 0.8)")
        print("   The feature projection may be over-smoothing spatial information.")
    else:
        print("\nâœ… OK: Patches have sufficient diversity")
    
    # ã‚¹ãƒ­ãƒƒãƒˆåˆæœŸåŒ–ã®ç¢ºèª
    print("\nðŸ” Slot initialization (mu, sigma):")
    print(f"  slots_mu shape: {model.slot_attention.slots_mu.shape}")
    print(f"  slots_mu mean: {model.slot_attention.slots_mu.mean().item():.6f}")
    print(f"  slots_mu std: {model.slot_attention.slots_mu.std().item():.6f}")
    print(f"  slots_mu min: {model.slot_attention.slots_mu.min().item():.6f}")
    print(f"  slots_mu max: {model.slot_attention.slots_mu.max().item():.6f}")
    
    print(f"\n  slots_log_sigma shape: {model.slot_attention.slots_log_sigma.shape}")
    print(f"  slots_log_sigma mean: {model.slot_attention.slots_log_sigma.mean().item():.6f}")
    print(f"  slots_log_sigma std: {model.slot_attention.slots_log_sigma.std().item():.6f}")
    
    # å®Ÿéš›ã«ã‚µãƒ³ãƒ—ãƒ«ã•ã‚Œã‚‹ã‚¹ãƒ­ãƒƒãƒˆã®åˆ†æ•£
    mu = model.slot_attention.slots_mu  # (1, 5, 64)
    sigma = model.slot_attention.slots_log_sigma.exp()  # (1, 1, 64)
    
    print(f"\n  sigma (exp of log_sigma):")
    print(f"    Mean: {sigma.mean().item():.6f}")
    print(f"    Std: {sigma.std().item():.6f}")
    print(f"    Min: {sigma.min().item():.6f}")
    print(f"    Max: {sigma.max().item():.6f}")
    
    # muã®å„ã‚¹ãƒ­ãƒƒãƒˆãŒã©ã‚Œãã‚‰ã„ç•°ãªã‚‹ã‹
    mu_flat = mu[0]  # (5, 64)
    mu_norm = torch.nn.functional.normalize(mu_flat, dim=1)
    mu_similarity = torch.mm(mu_norm, mu_norm.t())
    mask5 = torch.eye(5) == 0
    mu_off_diag = mu_similarity[mask5]
    
    print(f"\nðŸ“Š Slot mu similarity (how different are initial slot centers?):")
    print(f"  Mean: {mu_off_diag.mean().item():.6f}")
    print(f"  Std: {mu_off_diag.std().item():.6f}")
    print(f"  Min: {mu_off_diag.min().item():.6f}")
    print(f"  Max: {mu_off_diag.max().item():.6f}")
    
    if mu_off_diag.mean().item() > 0.9:
        print("\nâŒ PROBLEM: All slot mu are nearly identical (similarity > 0.9)")
        print("   Xavier initialization failed or was overridden!")
    else:
        print("\nâœ… OK: Slot mu have sufficient diversity")
