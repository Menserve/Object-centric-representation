{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e3a9a1",
   "metadata": {},
   "source": [
    "# Slot vs DINOSAUR: 同条件比較実験\n",
    "\n",
    "## 目的\n",
    "- ピクセルベースSlot Attention と DINOv2特徴ベースDINOSAURを**同条件**で比較\n",
    "- 色変化に対するマスク安定性を**定量評価**\n",
    "\n",
    "## 統一条件\n",
    "- 入力解像度: 224×224\n",
    "- 学習ステップ: 2000 (時間短縮のため)\n",
    "- スロット数: 5\n",
    "- テストデータ: 同一の4種類の色変換\n",
    "- 評価指標: マスク安定性 (色変化間のコサイン類似度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c46e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# ==========================================\n",
    "# 共通設定\n",
    "# ==========================================\n",
    "RESOLUTION = (224, 224)\n",
    "NUM_SLOTS = 5\n",
    "TOTAL_STEPS = 2000\n",
    "BATCH_SIZE = 24\n",
    "LR = 0.0004\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eea7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 共通データセット\n",
    "# ==========================================\n",
    "class JitterDogDataset(Dataset):\n",
    "    def __init__(self, resolution=RESOLUTION, length=10000):\n",
    "        self.resolution = resolution\n",
    "        self.length = length\n",
    "        self.raw_img = self._load_dog_image()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.5),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def _load_dog_image(self):\n",
    "        url = \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            return img.resize(self.resolution)\n",
    "        except:\n",
    "            print(\"Using noise.\")\n",
    "            return Image.fromarray(np.uint8(np.random.rand(*self.resolution, 3)*255))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.raw_img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "def create_test_batch(raw_img, device):\n",
    "    \"\"\"同一の4種類の色変換を適用したテストバッチを作成\"\"\"\n",
    "    img1 = TF.adjust_hue(raw_img, 0.45)\n",
    "    img1 = TF.adjust_saturation(img1, 1.5)  # Blue\n",
    "    img2 = TF.adjust_brightness(raw_img, 0.2)  # Dark\n",
    "    img3 = TF.adjust_hue(raw_img, -0.4)\n",
    "    img3 = TF.adjust_saturation(img3, 1.5)  # Purple\n",
    "    img4 = TF.adjust_contrast(raw_img, 3.0)  # High Contrast\n",
    "    \n",
    "    return torch.stack([\n",
    "        transforms.ToTensor()(img1),\n",
    "        transforms.ToTensor()(img2),\n",
    "        transforms.ToTensor()(img3),\n",
    "        transforms.ToTensor()(img4)\n",
    "    ]).to(device)\n",
    "\n",
    "dataset = JitterDogDataset(length=20000)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"Dataset loaded. Resolution: {RESOLUTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67851f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 評価指標: マスク安定性\n",
    "# ==========================================\n",
    "def compute_mask_stability(masks):\n",
    "    \"\"\"\n",
    "    4つの色変換画像のマスク間でコサイン類似度を計算\n",
    "    masks: (4, num_slots, H, W)\n",
    "    Returns: 平均コサイン類似度 (高いほど色に対してロバスト)\n",
    "    \"\"\"\n",
    "    num_images = masks.shape[0]\n",
    "    num_slots = masks.shape[1]\n",
    "    \n",
    "    # マスクをフラット化\n",
    "    masks_flat = masks.view(num_images, num_slots, -1)  # (4, 5, H*W)\n",
    "    \n",
    "    similarities = []\n",
    "    for i in range(num_images):\n",
    "        for j in range(i+1, num_images):\n",
    "            # 各スロットペアのコサイン類似度\n",
    "            for s in range(num_slots):\n",
    "                m1 = masks_flat[i, s]\n",
    "                m2 = masks_flat[j, s]\n",
    "                cos_sim = torch.nn.functional.cosine_similarity(m1.unsqueeze(0), m2.unsqueeze(0))\n",
    "                similarities.append(cos_sim.item())\n",
    "    \n",
    "    return np.mean(similarities), np.std(similarities)\n",
    "\n",
    "def compute_slot_diversity(masks):\n",
    "    \"\"\"\n",
    "    スロット間の多様性を計算（低いほど異なるスロットが異なる領域を担当）\n",
    "    \"\"\"\n",
    "    num_slots = masks.shape[1]\n",
    "    masks_flat = masks.view(-1, num_slots, masks.shape[-2] * masks.shape[-1])  # (B, 5, H*W)\n",
    "    \n",
    "    similarities = []\n",
    "    for b in range(masks_flat.shape[0]):\n",
    "        for i in range(num_slots):\n",
    "            for j in range(i+1, num_slots):\n",
    "                cos_sim = torch.nn.functional.cosine_similarity(\n",
    "                    masks_flat[b, i].unsqueeze(0), \n",
    "                    masks_flat[b, j].unsqueeze(0)\n",
    "                )\n",
    "                similarities.append(cos_sim.item())\n",
    "    \n",
    "    return np.mean(similarities), np.std(similarities)\n",
    "\n",
    "print(\"Evaluation metrics defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d874dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Model 1: Pixel-based Slot Attention (224x224対応)\n",
    "# ==========================================\n",
    "class SlotAttention(nn.Module):\n",
    "    def __init__(self, num_slots, dim, iters=5, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_slots = num_slots\n",
    "        self.iters = iters\n",
    "        self.scale = dim ** -0.5\n",
    "        self.to_q = nn.Linear(dim, dim)\n",
    "        self.to_k = nn.Linear(dim, dim)\n",
    "        self.to_v = nn.Linear(dim, dim)\n",
    "        self.gru = nn.GRUCell(dim, dim)\n",
    "        self.norm_input = nn.LayerNorm(dim)\n",
    "        self.norm_slots = nn.LayerNorm(dim)\n",
    "        self.norm_pre_ff = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim), nn.ReLU(inplace=True), nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, temperature=1.0):\n",
    "        b, n, d = inputs.shape\n",
    "        inputs = self.norm_input(inputs)\n",
    "        slots = torch.randn(b, self.num_slots, d, device=inputs.device)\n",
    "        k = self.to_k(inputs)\n",
    "        v = self.to_v(inputs)\n",
    "\n",
    "        for _ in range(self.iters):\n",
    "            slots_prev = slots\n",
    "            slots = self.norm_slots(slots)\n",
    "            q = self.to_q(slots)\n",
    "            dots = torch.einsum('bid,bjd->bij', q, k) * self.scale\n",
    "            attn = (dots / temperature).softmax(dim=1) + 1e-8\n",
    "            attn_sum = attn.sum(dim=-1, keepdim=True)\n",
    "            updates = torch.einsum('bjd,bij->bid', v, attn / attn_sum)\n",
    "            slots = self.gru(updates.reshape(-1, d), slots_prev.reshape(-1, d))\n",
    "            slots = slots.reshape(b, -1, d)\n",
    "            slots = slots + self.mlp(self.norm_pre_ff(slots))\n",
    "        return slots\n",
    "\n",
    "class PixelSlotModel(nn.Module):\n",
    "    \"\"\"224x224入力対応のピクセルベースSlot Attention\"\"\"\n",
    "    def __init__(self, resolution=RESOLUTION, num_slots=NUM_SLOTS, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.resolution = resolution\n",
    "        self.num_slots = num_slots\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Encoder with downsampling (224 -> 28)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(5, 64, 5, stride=2, padding=2), nn.ReLU(),  # 112\n",
    "            nn.Conv2d(64, 64, 5, stride=2, padding=2), nn.ReLU(),  # 56\n",
    "            nn.Conv2d(64, 64, 5, stride=2, padding=2), nn.ReLU(),  # 28\n",
    "            nn.Conv2d(64, hidden_dim, 5, padding=2), nn.ReLU(),\n",
    "        )\n",
    "        self.feat_size = 28\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, hidden_dim, self.feat_size, self.feat_size) * 0.01)\n",
    "        self.slot_attention = SlotAttention(num_slots, hidden_dim, iters=5, hidden_dim=128)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim + 2, 64, 5, padding=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 5, padding=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 4, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def build_grid(self, batch_size, resolution, device):\n",
    "        h, w = resolution\n",
    "        x = torch.linspace(-1, 1, w, device=device)\n",
    "        y = torch.linspace(-1, 1, h, device=device)\n",
    "        grid_y, grid_x = torch.meshgrid(y, x, indexing='ij')\n",
    "        return torch.stack((grid_x, grid_y), dim=0).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "    def forward(self, x, temperature=1.0):\n",
    "        b, c, h, w = x.shape\n",
    "        grid_in = self.build_grid(b, (h, w), x.device)\n",
    "        x_with_grid = torch.cat([x, grid_in], dim=1)\n",
    "        \n",
    "        x_enc = self.encoder(x_with_grid) + self.pos_emb\n",
    "        feat_h, feat_w = x_enc.shape[2], x_enc.shape[3]\n",
    "        x_flat = x_enc.permute(0, 2, 3, 1).reshape(b, -1, self.hidden_dim)\n",
    "        \n",
    "        slots = self.slot_attention(x_flat, temperature)\n",
    "        \n",
    "        slots_img = slots.view(b * self.num_slots, self.hidden_dim, 1, 1).repeat(1, 1, feat_h, feat_w)\n",
    "        grid_dec = self.build_grid(b * self.num_slots, (feat_h, feat_w), x.device)\n",
    "        out = self.decoder(torch.cat([slots_img, grid_dec], dim=1))\n",
    "        out = out.view(b, self.num_slots, 4, feat_h, feat_w)\n",
    "        \n",
    "        recons = out[:, :, :3, :, :]\n",
    "        masks = torch.softmax(out[:, :, 3:4, :, :] / temperature, dim=1)\n",
    "        recon_combined = torch.sum(recons * masks, dim=1)\n",
    "        \n",
    "        # Upsample to original resolution for loss\n",
    "        recon_up = nn.functional.interpolate(recon_combined, size=(h, w), mode='bilinear')\n",
    "        \n",
    "        return recon_up, masks\n",
    "\n",
    "print(\"PixelSlotModel defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Model 2: DINOSAUR (DINOv2 Feature-based)\n",
    "# ==========================================\n",
    "class DinoFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Loading DINOv2 model...\")\n",
    "        self.dino = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "        for param in self.dino.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.dino.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.dino.forward_features(x)[\"x_norm_patchtokens\"]\n",
    "        b, n, d = features.shape\n",
    "        h = w = int(n**0.5)\n",
    "        return features.permute(0, 2, 1).reshape(b, d, h, w)\n",
    "\n",
    "class DinosaurModel(nn.Module):\n",
    "    def __init__(self, num_slots=NUM_SLOTS):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = DinoFeatureExtractor()\n",
    "        self.feat_dim = 384\n",
    "        self.num_slots = num_slots\n",
    "        self.feat_size = 16\n",
    "        \n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, self.feat_size, self.feat_size, self.feat_dim) * 0.05)\n",
    "        self.slot_attention = SlotAttention(num_slots, self.feat_dim, iters=5, hidden_dim=512)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(self.feat_dim + 2, 384, 5, padding=2), nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, 5, padding=2), nn.ReLU(),\n",
    "            nn.Conv2d(384, self.feat_dim + 1, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def build_grid(self, batch_size, resolution, device):\n",
    "        h, w = resolution\n",
    "        x = torch.linspace(-1, 1, w, device=device)\n",
    "        y = torch.linspace(-1, 1, h, device=device)\n",
    "        grid_y, grid_x = torch.meshgrid(y, x, indexing='ij')\n",
    "        return torch.stack((grid_x, grid_y), dim=0).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_extractor(img)\n",
    "        b, c, h, w = features.shape\n",
    "        \n",
    "        features_pos = features.permute(0, 2, 3, 1) + self.pos_emb\n",
    "        features_flat = features_pos.reshape(b, -1, c)\n",
    "        \n",
    "        slots = self.slot_attention(features_flat)\n",
    "        \n",
    "        slots_2d = slots.view(b * self.num_slots, self.feat_dim, 1, 1).repeat(1, 1, h, w)\n",
    "        grid = self.build_grid(b * self.num_slots, (h, w), img.device)\n",
    "        \n",
    "        out = self.decoder(torch.cat([slots_2d, grid], dim=1))\n",
    "        out = out.view(b, self.num_slots, self.feat_dim + 1, h, w)\n",
    "        \n",
    "        pred_feats = out[:, :, :self.feat_dim, :, :]\n",
    "        masks = torch.softmax(out[:, :, self.feat_dim:, :, :], dim=1)\n",
    "        \n",
    "        recon_combined = torch.sum(pred_feats * masks, dim=1)\n",
    "        return recon_combined, features, masks\n",
    "\n",
    "print(\"DinosaurModel defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51527026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 学習・評価関数\n",
    "# ==========================================\n",
    "def train_pixel_model():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training: Pixel-based Slot Attention\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model = PixelSlotModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    model.train()\n",
    "    \n",
    "    start_temp, end_temp = 1.0, 0.01\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        progress = min(1.0, i / TOTAL_STEPS)\n",
    "        temp = start_temp * (1 - progress) + end_temp * progress\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon, _ = model(batch, temp)\n",
    "        loss = ((batch - recon)**2).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(f\"Step {i}: Loss {loss.item():.6f}\")\n",
    "        if i >= TOTAL_STEPS:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_dino_model():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training: DINOSAUR (DINOv2 Feature-based)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model = DinosaurModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_feat, target_feat, _ = model(batch)\n",
    "        loss = ((target_feat - recon_feat)**2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(f\"Step {i}: Loss {loss.item():.6f}\")\n",
    "        if i >= TOTAL_STEPS:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Training functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 実行: 両モデルを学習\n",
    "# ==========================================\n",
    "pixel_model = train_pixel_model()\n",
    "dino_model = train_dino_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8953c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 評価: 同一テストデータで比較\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Evaluation on Fixed Color Transformations\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_batch = create_test_batch(dataset.raw_img, device)\n",
    "\n",
    "# Pixel model evaluation\n",
    "pixel_model.eval()\n",
    "with torch.no_grad():\n",
    "    _, masks_pixel = pixel_model(test_batch, temperature=0.01)\n",
    "    # Upsample to common size\n",
    "    masks_pixel_up = nn.functional.interpolate(\n",
    "        masks_pixel.view(-1, 1, 28, 28), size=(224, 224), mode='bilinear'\n",
    "    ).view(4, NUM_SLOTS, 224, 224)\n",
    "\n",
    "# DINO model evaluation\n",
    "dino_model.eval()\n",
    "with torch.no_grad():\n",
    "    _, _, masks_dino = dino_model(test_batch)\n",
    "    masks_dino_up = nn.functional.interpolate(\n",
    "        masks_dino.view(-1, 1, 16, 16), size=(224, 224), mode='bilinear'\n",
    "    ).view(4, NUM_SLOTS, 224, 224)\n",
    "\n",
    "# Compute metrics\n",
    "stability_pixel, std_pixel = compute_mask_stability(masks_pixel_up)\n",
    "stability_dino, std_dino = compute_mask_stability(masks_dino_up)\n",
    "\n",
    "diversity_pixel, div_std_pixel = compute_slot_diversity(masks_pixel_up)\n",
    "diversity_dino, div_std_dino = compute_slot_diversity(masks_dino_up)\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'Pixel Slot':<20} {'DINOSAUR':<20}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Mask Stability (↑ better)':<25} {stability_pixel:.4f} ± {std_pixel:.4f}    {stability_dino:.4f} ± {std_dino:.4f}\")\n",
    "print(f\"{'Slot Diversity (↓ better)':<25} {diversity_pixel:.4f} ± {div_std_pixel:.4f}    {diversity_dino:.4f} ± {div_std_dino:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c004b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 可視化: 並列比較\n",
    "# ==========================================\n",
    "def to_np(t): \n",
    "    return t.permute(1, 2, 0).cpu().numpy().clip(0, 1)\n",
    "\n",
    "titles = [\"Blue\", \"Dark\", \"Purple\", \"High Contrast\"]\n",
    "\n",
    "fig, axes = plt.subplots(4, 12, figsize=(24, 10))\n",
    "fig.suptitle(f\"Pixel Slot (Stability={stability_pixel:.3f}) vs DINOSAUR (Stability={stability_dino:.3f})\", fontsize=14)\n",
    "\n",
    "for row in range(4):\n",
    "    # Input\n",
    "    axes[row, 0].imshow(to_np(test_batch[row]))\n",
    "    axes[row, 0].set_title(titles[row] if row == 0 else \"\")\n",
    "    axes[row, 0].set_ylabel(titles[row], fontsize=10)\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # Pixel Slot masks\n",
    "    for k in range(NUM_SLOTS):\n",
    "        axes[row, k+1].imshow(masks_pixel_up[row, k].cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "        if row == 0:\n",
    "            axes[row, k+1].set_title(f\"P-Slot{k+1}\")\n",
    "        axes[row, k+1].axis('off')\n",
    "    \n",
    "    # Separator\n",
    "    axes[row, 6].axis('off')\n",
    "    \n",
    "    # DINO masks\n",
    "    for k in range(NUM_SLOTS):\n",
    "        axes[row, k+7].imshow(masks_dino_up[row, k].cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "        if row == 0:\n",
    "            axes[row, k+7].set_title(f\"D-Slot{k+1}\")\n",
    "        axes[row, k+7].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_result.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nResults saved to comparison_result.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd501f7",
   "metadata": {},
   "source": [
    "## 結論\n",
    "\n",
    "### 定量評価結果\n",
    "- **Mask Stability**: 色変化に対するマスクの一貫性（高いほど良い）\n",
    "- **Slot Diversity**: スロット間の分離度（低いほど良い）\n",
    "\n",
    "### 考察\n",
    "- ピクセルベースSlotはRGB値を直接最適化するため、色変化に敏感\n",
    "- DINOSAURは事前学習済み特徴を使用するため、色・照明の変動に対してロバスト\n",
    "- これは物体中心表現学習において、事前学習特徴の有効性を示唆する"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
