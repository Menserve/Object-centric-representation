nohup: ignoring input
/home/menserve/Object-centric-representation/.venv/lib/python3.12/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).
  warnings.warn(
Device: cuda
Backbone: clip_vitb16

1. Loading MOVi-A dataset...
MoviDataset: Found 60 samples (split=all)
Train samples: 58
Test samples: 2

2. Creating SAVi-DINOSAUR model with clip_vitb16...
Loading clip_vitb16 model...
Mask temperature (τ): 0.5
Trainable parameters: 9,321,665

3. Training...

============================================================
Training SAVi-DINOSAUR on MOVi-A
============================================================
Device: cuda
Epochs: 200
Learning rate: 0.0004 (with 5-epoch warmup)
Batch size: 2
Dataset size: 58
Diversity weight: 0.1
============================================================

  Batch 0/29: Loss = 1.076437
  Batch 10/29: Loss = 1.071727
  Batch 20/29: Loss = 1.060690

Epoch 1/200 | Loss: 1.061755 | LR: 0.000083 | Time: 1.4s

  ✓ Saved best model (loss=1.061755)
  Batch 0/29: Loss = 1.043738
  Batch 10/29: Loss = 0.821965
  Batch 20/29: Loss = 0.719782

Epoch 2/200 | Loss: 0.761415 | LR: 0.000162 | Time: 0.6s

  ✓ Saved best model (loss=0.761415)
  Batch 0/29: Loss = 0.558173
  Batch 10/29: Loss = 0.507625
  Batch 20/29: Loss = 0.420733

Epoch 3/200 | Loss: 0.479023 | LR: 0.000242 | Time: 0.8s

  ✓ Saved best model (loss=0.479023)
  Batch 0/29: Loss = 0.332089
  Batch 10/29: Loss = 0.347773
  Batch 20/29: Loss = 0.297820

Epoch 4/200 | Loss: 0.339240 | LR: 0.000321 | Time: 0.6s

  ✓ Saved best model (loss=0.339240)
  Batch 0/29: Loss = 0.235186
  Batch 10/29: Loss = 0.190603
  Batch 20/29: Loss = 0.212317

Epoch 5/200 | Loss: 0.237883 | LR: 0.000400 | Time: 0.8s

  ✓ Saved best model (loss=0.237883)
  Batch 0/29: Loss = 0.217697
  Batch 10/29: Loss = 0.185734
  Batch 20/29: Loss = 0.129099

Epoch 6/200 | Loss: 0.188117 | LR: 0.000400 | Time: 0.9s

  ✓ Saved best model (loss=0.188117)
  Batch 0/29: Loss = 0.170024
  Batch 10/29: Loss = 0.223418
  Batch 20/29: Loss = 0.100371

Epoch 7/200 | Loss: 0.162750 | LR: 0.000400 | Time: 0.9s

  ✓ Saved best model (loss=0.162750)
  Batch 0/29: Loss = 0.153487
  Batch 10/29: Loss = 0.207231
  Batch 20/29: Loss = 0.137384

Epoch 8/200 | Loss: 0.140779 | LR: 0.000400 | Time: 1.0s

  ✓ Saved best model (loss=0.140779)
  Batch 0/29: Loss = 0.153259
  Batch 10/29: Loss = 0.121877
  Batch 20/29: Loss = 0.104132

Epoch 9/200 | Loss: 0.128907 | LR: 0.000400 | Time: 0.9s

  ✓ Saved best model (loss=0.128907)
  Batch 0/29: Loss = 0.154538
  Batch 10/29: Loss = 0.108613
  Batch 20/29: Loss = 0.101956

Epoch 10/200 | Loss: 0.116999 | LR: 0.000399 | Time: 0.9s

  ✓ Saved best model (loss=0.116999)
  Batch 0/29: Loss = 0.100214
  Batch 10/29: Loss = 0.138609
  Batch 20/29: Loss = 0.139871

Epoch 11/200 | Loss: 0.112848 | LR: 0.000399 | Time: 1.6s

  ✓ Saved best model (loss=0.112848)
  Batch 0/29: Loss = 0.081457
  Batch 10/29: Loss = 0.132460
  Batch 20/29: Loss = 0.113677

Epoch 12/200 | Loss: 0.101026 | LR: 0.000399 | Time: 0.7s

  ✓ Saved best model (loss=0.101026)
  Batch 0/29: Loss = 0.092593
  Batch 10/29: Loss = 0.115580
  Batch 20/29: Loss = 0.102111

Epoch 13/200 | Loss: 0.087678 | LR: 0.000398 | Time: 0.9s

  ✓ Saved best model (loss=0.087678)
  Batch 0/29: Loss = 0.065472
  Batch 10/29: Loss = 0.088616
  Batch 20/29: Loss = 0.077173

Epoch 14/200 | Loss: 0.080785 | LR: 0.000398 | Time: 0.9s

  ✓ Saved best model (loss=0.080785)
  Batch 0/29: Loss = 0.077710
  Batch 10/29: Loss = 0.071831
  Batch 20/29: Loss = 0.074027

Epoch 15/200 | Loss: 0.081457 | LR: 0.000397 | Time: 0.9s

  Batch 0/29: Loss = 0.096261
  Batch 10/29: Loss = 0.059487
  Batch 20/29: Loss = 0.073977

Epoch 16/200 | Loss: 0.078606 | LR: 0.000397 | Time: 0.9s

  ✓ Saved best model (loss=0.078606)
  Batch 0/29: Loss = 0.058575
  Batch 10/29: Loss = 0.048616
  Batch 20/29: Loss = 0.085945

Epoch 17/200 | Loss: 0.069584 | LR: 0.000396 | Time: 0.9s

  ✓ Saved best model (loss=0.069584)
  Batch 0/29: Loss = 0.078641
  Batch 10/29: Loss = 0.054914
  Batch 20/29: Loss = 0.058102

Epoch 18/200 | Loss: 0.071279 | LR: 0.000396 | Time: 0.9s

  Batch 0/29: Loss = 0.106038
  Batch 10/29: Loss = 0.078780
  Batch 20/29: Loss = 0.059939

Epoch 19/200 | Loss: 0.065101 | LR: 0.000395 | Time: 1.0s

  ✓ Saved best model (loss=0.065101)
  Batch 0/29: Loss = 0.072767
  Batch 10/29: Loss = 0.048018
  Batch 20/29: Loss = 0.047510

Epoch 20/200 | Loss: 0.062127 | LR: 0.000394 | Time: 0.9s

  ✓ Saved best model (loss=0.062127)
  Batch 0/29: Loss = 0.042585
  Batch 10/29: Loss = 0.065931
  Batch 20/29: Loss = 0.067986

Epoch 21/200 | Loss: 0.062008 | LR: 0.000393 | Time: 1.0s

  ✓ Saved best model (loss=0.062008)
  Batch 0/29: Loss = 0.089766
  Batch 10/29: Loss = 0.044360
  Batch 20/29: Loss = 0.045557

Epoch 22/200 | Loss: 0.057085 | LR: 0.000393 | Time: 1.0s

  ✓ Saved best model (loss=0.057085)
  Batch 0/29: Loss = 0.062578
  Batch 10/29: Loss = 0.066664
  Batch 20/29: Loss = 0.044714

Epoch 23/200 | Loss: 0.056005 | LR: 0.000392 | Time: 0.9s

  ✓ Saved best model (loss=0.056005)
  Batch 0/29: Loss = 0.055138
  Batch 10/29: Loss = 0.081334
  Batch 20/29: Loss = 0.060574

Epoch 24/200 | Loss: 0.059175 | LR: 0.000391 | Time: 0.8s

  Batch 0/29: Loss = 0.067842
  Batch 10/29: Loss = 0.052840
  Batch 20/29: Loss = 0.050643

Epoch 25/200 | Loss: 0.059290 | LR: 0.000390 | Time: 0.9s

  Batch 0/29: Loss = 0.073438
  Batch 10/29: Loss = 0.066414
  Batch 20/29: Loss = 0.045061

Epoch 26/200 | Loss: 0.058118 | LR: 0.000389 | Time: 0.9s

  Batch 0/29: Loss = 0.051670
  Batch 10/29: Loss = 0.062928
  Batch 20/29: Loss = 0.069330

Epoch 27/200 | Loss: 0.054188 | LR: 0.000388 | Time: 0.9s

  ✓ Saved best model (loss=0.054188)
  Batch 0/29: Loss = 0.043188
  Batch 10/29: Loss = 0.028528
  Batch 20/29: Loss = 0.080930

Epoch 28/200 | Loss: 0.049034 | LR: 0.000386 | Time: 0.9s

  ✓ Saved best model (loss=0.049034)
  Batch 0/29: Loss = 0.066364
  Batch 10/29: Loss = 0.062892
  Batch 20/29: Loss = 0.058302

Epoch 29/200 | Loss: 0.050599 | LR: 0.000385 | Time: 0.9s

  Batch 0/29: Loss = 0.051157
  Batch 10/29: Loss = 0.063297
  Batch 20/29: Loss = 0.040809

Epoch 30/200 | Loss: 0.048305 | LR: 0.000384 | Time: 1.0s

  ✓ Saved best model (loss=0.048305)
  Batch 0/29: Loss = 0.027481
  Batch 10/29: Loss = 0.039190
  Batch 20/29: Loss = 0.081927

Epoch 31/200 | Loss: 0.046717 | LR: 0.000383 | Time: 0.8s

  ✓ Saved best model (loss=0.046717)
  Batch 0/29: Loss = 0.060348
  Batch 10/29: Loss = 0.062193
  Batch 20/29: Loss = 0.030376

Epoch 32/200 | Loss: 0.046497 | LR: 0.000381 | Time: 0.9s

  ✓ Saved best model (loss=0.046497)
  Batch 0/29: Loss = 0.049226
  Batch 10/29: Loss = 0.043815
  Batch 20/29: Loss = 0.033332

Epoch 33/200 | Loss: 0.047872 | LR: 0.000380 | Time: 1.0s

  Batch 0/29: Loss = 0.025563
  Batch 10/29: Loss = 0.046578
  Batch 20/29: Loss = 0.073154

Epoch 34/200 | Loss: 0.045584 | LR: 0.000379 | Time: 1.0s

  ✓ Saved best model (loss=0.045584)
  Batch 0/29: Loss = 0.047591
  Batch 10/29: Loss = 0.030311
  Batch 20/29: Loss = 0.045890

Epoch 35/200 | Loss: 0.048030 | LR: 0.000377 | Time: 0.9s

  Batch 0/29: Loss = 0.034422
  Batch 10/29: Loss = 0.052202
  Batch 20/29: Loss = 0.062790

Epoch 36/200 | Loss: 0.050627 | LR: 0.000376 | Time: 0.9s

  Batch 0/29: Loss = 0.035362
  Batch 10/29: Loss = 0.029289
  Batch 20/29: Loss = 0.077551

Epoch 37/200 | Loss: 0.048473 | LR: 0.000374 | Time: 0.9s

  Batch 0/29: Loss = 0.028676
  Batch 10/29: Loss = 0.047912
  Batch 20/29: Loss = 0.057612

Epoch 38/200 | Loss: 0.048347 | LR: 0.000372 | Time: 1.0s

  Batch 0/29: Loss = 0.067000
  Batch 10/29: Loss = 0.047333
  Batch 20/29: Loss = 0.051608

Epoch 39/200 | Loss: 0.046345 | LR: 0.000371 | Time: 1.0s

  Batch 0/29: Loss = 0.042026
  Batch 10/29: Loss = 0.079960
  Batch 20/29: Loss = 0.034800

Epoch 40/200 | Loss: 0.043392 | LR: 0.000369 | Time: 0.9s

  ✓ Saved best model (loss=0.043392)
  Batch 0/29: Loss = 0.047547
  Batch 10/29: Loss = 0.030993
  Batch 20/29: Loss = 0.070787

Epoch 41/200 | Loss: 0.045577 | LR: 0.000367 | Time: 1.0s

  Batch 0/29: Loss = 0.027297
  Batch 10/29: Loss = 0.037772
  Batch 20/29: Loss = 0.065345

Epoch 42/200 | Loss: 0.040818 | LR: 0.000366 | Time: 0.9s

  ✓ Saved best model (loss=0.040818)
  Batch 0/29: Loss = 0.018013
  Batch 10/29: Loss = 0.042398
  Batch 20/29: Loss = 0.025711

Epoch 43/200 | Loss: 0.043844 | LR: 0.000364 | Time: 0.9s

  Batch 0/29: Loss = 0.022124
  Batch 10/29: Loss = 0.038704
  Batch 20/29: Loss = 0.045665

Epoch 44/200 | Loss: 0.042320 | LR: 0.000362 | Time: 1.0s

  Batch 0/29: Loss = 0.062201
  Batch 10/29: Loss = 0.061110
  Batch 20/29: Loss = 0.030941

Epoch 45/200 | Loss: 0.045028 | LR: 0.000360 | Time: 0.9s

  Batch 0/29: Loss = 0.052893
  Batch 10/29: Loss = 0.059112
  Batch 20/29: Loss = 0.030519

Epoch 46/200 | Loss: 0.043941 | LR: 0.000358 | Time: 0.9s

  Batch 0/29: Loss = 0.036810
  Batch 10/29: Loss = 0.030253
  Batch 20/29: Loss = 0.051814

Epoch 47/200 | Loss: 0.044268 | LR: 0.000356 | Time: 0.8s

  Batch 0/29: Loss = 0.018048
  Batch 10/29: Loss = 0.030182
  Batch 20/29: Loss = 0.028152

Epoch 48/200 | Loss: 0.046937 | LR: 0.000354 | Time: 0.9s

  Batch 0/29: Loss = 0.036919
  Batch 10/29: Loss = 0.050505
  Batch 20/29: Loss = 0.042306

Epoch 49/200 | Loss: 0.046742 | LR: 0.000352 | Time: 0.9s

  Batch 0/29: Loss = 0.020215
  Batch 10/29: Loss = 0.017573
  Batch 20/29: Loss = 0.044592

Epoch 50/200 | Loss: 0.040630 | LR: 0.000350 | Time: 1.1s

  ✓ Saved best model (loss=0.040630)
  Batch 0/29: Loss = 0.039571
  Batch 10/29: Loss = 0.029185
  Batch 20/29: Loss = 0.053372

Epoch 51/200 | Loss: 0.037255 | LR: 0.000348 | Time: 1.0s

  ✓ Saved best model (loss=0.037255)
  Batch 0/29: Loss = 0.024995
  Batch 10/29: Loss = 0.041538
  Batch 20/29: Loss = 0.026994

Epoch 52/200 | Loss: 0.042226 | LR: 0.000345 | Time: 1.0s

  Batch 0/29: Loss = 0.070212
  Batch 10/29: Loss = 0.027859
  Batch 20/29: Loss = 0.043373

Epoch 53/200 | Loss: 0.037894 | LR: 0.000343 | Time: 1.0s

  Batch 0/29: Loss = 0.025172
  Batch 10/29: Loss = 0.044857
  Batch 20/29: Loss = 0.026311

Epoch 54/200 | Loss: 0.040492 | LR: 0.000341 | Time: 0.9s

  Batch 0/29: Loss = 0.038784
  Batch 10/29: Loss = 0.050062
  Batch 20/29: Loss = 0.051202

Epoch 55/200 | Loss: 0.041352 | LR: 0.000339 | Time: 0.9s

  Batch 0/29: Loss = 0.031625
  Batch 10/29: Loss = 0.025092
  Batch 20/29: Loss = 0.028429

Epoch 56/200 | Loss: 0.040133 | LR: 0.000336 | Time: 1.0s

  Batch 0/29: Loss = 0.033662
  Batch 10/29: Loss = 0.016920
  Batch 20/29: Loss = 0.035081

Epoch 57/200 | Loss: 0.036799 | LR: 0.000334 | Time: 1.0s

  ✓ Saved best model (loss=0.036799)
  Batch 0/29: Loss = 0.020424
  Batch 10/29: Loss = 0.028279
  Batch 20/29: Loss = 0.034115

Epoch 58/200 | Loss: 0.040087 | LR: 0.000331 | Time: 1.0s

  Batch 0/29: Loss = 0.040182
  Batch 10/29: Loss = 0.023380
  Batch 20/29: Loss = 0.024059

Epoch 59/200 | Loss: 0.042009 | LR: 0.000329 | Time: 1.0s

  Batch 0/29: Loss = 0.037478
  Batch 10/29: Loss = 0.042424
  Batch 20/29: Loss = 0.039756

Epoch 60/200 | Loss: 0.039498 | LR: 0.000326 | Time: 1.0s

  Batch 0/29: Loss = 0.077307
  Batch 10/29: Loss = 0.052197
  Batch 20/29: Loss = 0.012793

Epoch 61/200 | Loss: 0.036704 | LR: 0.000324 | Time: 0.9s

  ✓ Saved best model (loss=0.036704)
  Batch 0/29: Loss = 0.038959
  Batch 10/29: Loss = 0.025027
  Batch 20/29: Loss = 0.029729

Epoch 62/200 | Loss: 0.040237 | LR: 0.000321 | Time: 1.0s

  Batch 0/29: Loss = 0.019885
  Batch 10/29: Loss = 0.046076
  Batch 20/29: Loss = 0.018143

Epoch 63/200 | Loss: 0.039627 | LR: 0.000319 | Time: 1.0s

  Batch 0/29: Loss = 0.026760
  Batch 10/29: Loss = 0.050324
  Batch 20/29: Loss = 0.042373

Epoch 64/200 | Loss: 0.037558 | LR: 0.000316 | Time: 1.6s

  Batch 0/29: Loss = 0.039873
  Batch 10/29: Loss = 0.037211
  Batch 20/29: Loss = 0.023257

Epoch 65/200 | Loss: 0.031470 | LR: 0.000314 | Time: 0.9s

  ✓ Saved best model (loss=0.031470)
  Batch 0/29: Loss = 0.043941
  Batch 10/29: Loss = 0.034657
  Batch 20/29: Loss = 0.036142

Epoch 66/200 | Loss: 0.035528 | LR: 0.000311 | Time: 1.0s

  Batch 0/29: Loss = 0.023264
  Batch 10/29: Loss = 0.020940
  Batch 20/29: Loss = 0.012632

Epoch 67/200 | Loss: 0.036542 | LR: 0.000308 | Time: 1.0s

  Batch 0/29: Loss = 0.052942
  Batch 10/29: Loss = 0.041568
  Batch 20/29: Loss = 0.101865

Epoch 68/200 | Loss: 0.038838 | LR: 0.000306 | Time: 0.9s

  Batch 0/29: Loss = 0.062795
  Batch 10/29: Loss = 0.035040
  Batch 20/29: Loss = 0.030344

Epoch 69/200 | Loss: 0.034137 | LR: 0.000303 | Time: 0.9s

  Batch 0/29: Loss = 0.034114
  Batch 10/29: Loss = 0.009852
  Batch 20/29: Loss = 0.044719

Epoch 70/200 | Loss: 0.034320 | LR: 0.000300 | Time: 0.8s

  Batch 0/29: Loss = 0.031029
  Batch 10/29: Loss = 0.045725
  Batch 20/29: Loss = 0.036692

Epoch 71/200 | Loss: 0.033824 | LR: 0.000297 | Time: 1.0s

  Batch 0/29: Loss = 0.036434
  Batch 10/29: Loss = 0.077341
  Batch 20/29: Loss = 0.014589

Epoch 72/200 | Loss: 0.031140 | LR: 0.000294 | Time: 0.9s

  ✓ Saved best model (loss=0.031140)
  Batch 0/29: Loss = 0.063000
  Batch 10/29: Loss = 0.043352
  Batch 20/29: Loss = 0.041567

Epoch 73/200 | Loss: 0.036479 | LR: 0.000292 | Time: 1.0s

  Batch 0/29: Loss = 0.060660
  Batch 10/29: Loss = 0.013981
  Batch 20/29: Loss = 0.015487

Epoch 74/200 | Loss: 0.041180 | LR: 0.000289 | Time: 0.9s

  Batch 0/29: Loss = 0.049482
  Batch 10/29: Loss = 0.036192
  Batch 20/29: Loss = 0.027993

Epoch 75/200 | Loss: 0.032176 | LR: 0.000286 | Time: 0.9s

  Batch 0/29: Loss = 0.052396
  Batch 10/29: Loss = 0.031449
  Batch 20/29: Loss = 0.043101

Epoch 76/200 | Loss: 0.031847 | LR: 0.000283 | Time: 0.9s

  Batch 0/29: Loss = 0.041680
  Batch 10/29: Loss = 0.023398
  Batch 20/29: Loss = 0.077566

Epoch 77/200 | Loss: 0.036259 | LR: 0.000280 | Time: 1.0s

  Batch 0/29: Loss = 0.049435
  Batch 10/29: Loss = 0.033329
  Batch 20/29: Loss = 0.013106

Epoch 78/200 | Loss: 0.035680 | LR: 0.000277 | Time: 0.9s

  Batch 0/29: Loss = 0.091680
  Batch 10/29: Loss = 0.038397
  Batch 20/29: Loss = 0.059034

Epoch 79/200 | Loss: 0.036607 | LR: 0.000274 | Time: 0.9s

  Batch 0/29: Loss = 0.052863
  Batch 10/29: Loss = 0.050662
  Batch 20/29: Loss = 0.044518

Epoch 80/200 | Loss: 0.035062 | LR: 0.000271 | Time: 1.0s

  Batch 0/29: Loss = 0.020432
  Batch 10/29: Loss = 0.046638
  Batch 20/29: Loss = 0.045298

Epoch 81/200 | Loss: 0.035189 | LR: 0.000268 | Time: 1.0s

  Batch 0/29: Loss = 0.028468
  Batch 10/29: Loss = 0.025348
  Batch 20/29: Loss = 0.048838

Epoch 82/200 | Loss: 0.034759 | LR: 0.000265 | Time: 0.9s

  Batch 0/29: Loss = 0.069886
  Batch 10/29: Loss = 0.028183
  Batch 20/29: Loss = 0.025531

Epoch 83/200 | Loss: 0.032750 | LR: 0.000262 | Time: 1.0s

  Batch 0/29: Loss = 0.047573
  Batch 10/29: Loss = 0.033106
  Batch 20/29: Loss = 0.024916

Epoch 84/200 | Loss: 0.035325 | LR: 0.000259 | Time: 1.0s

  Batch 0/29: Loss = 0.050285
  Batch 10/29: Loss = 0.047478
  Batch 20/29: Loss = 0.037755

Epoch 85/200 | Loss: 0.033443 | LR: 0.000256 | Time: 1.0s

  Batch 0/29: Loss = 0.040094
  Batch 10/29: Loss = 0.034753
  Batch 20/29: Loss = 0.035044

Epoch 86/200 | Loss: 0.032456 | LR: 0.000253 | Time: 0.8s

  Batch 0/29: Loss = 0.031412
  Batch 10/29: Loss = 0.022810
  Batch 20/29: Loss = 0.015615

Epoch 87/200 | Loss: 0.033623 | LR: 0.000249 | Time: 0.8s

  Batch 0/29: Loss = 0.021930
  Batch 10/29: Loss = 0.025792
  Batch 20/29: Loss = 0.066529

Epoch 88/200 | Loss: 0.034651 | LR: 0.000246 | Time: 1.0s

  Batch 0/29: Loss = 0.021595
  Batch 10/29: Loss = 0.039649
  Batch 20/29: Loss = 0.030853

Epoch 89/200 | Loss: 0.030586 | LR: 0.000243 | Time: 1.0s

  ✓ Saved best model (loss=0.030586)
  Batch 0/29: Loss = 0.032970
  Batch 10/29: Loss = 0.049572
  Batch 20/29: Loss = 0.019981

Epoch 90/200 | Loss: 0.033103 | LR: 0.000240 | Time: 0.9s

  Batch 0/29: Loss = 0.020591
  Batch 10/29: Loss = 0.024353
  Batch 20/29: Loss = 0.035777

Epoch 91/200 | Loss: 0.031821 | LR: 0.000237 | Time: 0.9s

  Batch 0/29: Loss = 0.011772
  Batch 10/29: Loss = 0.043286
  Batch 20/29: Loss = 0.023116

Epoch 92/200 | Loss: 0.028476 | LR: 0.000234 | Time: 0.8s

  ✓ Saved best model (loss=0.028476)
  Batch 0/29: Loss = 0.049597
  Batch 10/29: Loss = 0.041004
  Batch 20/29: Loss = 0.061787

Epoch 93/200 | Loss: 0.032734 | LR: 0.000230 | Time: 1.0s

  Batch 0/29: Loss = 0.039636
  Batch 10/29: Loss = 0.019308
  Batch 20/29: Loss = 0.026096

Epoch 94/200 | Loss: 0.030447 | LR: 0.000227 | Time: 0.9s

  Batch 0/29: Loss = 0.018033
  Batch 10/29: Loss = 0.027268
  Batch 20/29: Loss = 0.016305

Epoch 95/200 | Loss: 0.032344 | LR: 0.000224 | Time: 0.8s

  Batch 0/29: Loss = 0.029860
  Batch 10/29: Loss = 0.049447
  Batch 20/29: Loss = 0.034276

Epoch 96/200 | Loss: 0.031737 | LR: 0.000221 | Time: 1.7s

  Batch 0/29: Loss = 0.034370
  Batch 10/29: Loss = 0.055676
  Batch 20/29: Loss = 0.019167

Epoch 97/200 | Loss: 0.030858 | LR: 0.000218 | Time: 1.0s

  Batch 0/29: Loss = 0.016665
  Batch 10/29: Loss = 0.016545
  Batch 20/29: Loss = 0.038982

Epoch 98/200 | Loss: 0.034756 | LR: 0.000214 | Time: 1.0s

  Batch 0/29: Loss = 0.022446
  Batch 10/29: Loss = 0.016707
  Batch 20/29: Loss = 0.035161

Epoch 99/200 | Loss: 0.031722 | LR: 0.000211 | Time: 0.9s

  Batch 0/29: Loss = 0.018170
  Batch 10/29: Loss = 0.045088
  Batch 20/29: Loss = 0.044686

Epoch 100/200 | Loss: 0.034257 | LR: 0.000208 | Time: 0.9s

  Batch 0/29: Loss = 0.015040
  Batch 10/29: Loss = 0.017584
  Batch 20/29: Loss = 0.044872

Epoch 101/200 | Loss: 0.029945 | LR: 0.000205 | Time: 1.0s

  Batch 0/29: Loss = 0.053467
  Batch 10/29: Loss = 0.026080
  Batch 20/29: Loss = 0.015338

Epoch 102/200 | Loss: 0.031811 | LR: 0.000202 | Time: 0.9s

  Batch 0/29: Loss = 0.017907
  Batch 10/29: Loss = 0.043141
  Batch 20/29: Loss = 0.020734

Epoch 103/200 | Loss: 0.034922 | LR: 0.000198 | Time: 0.9s

  Batch 0/29: Loss = 0.051062
  Batch 10/29: Loss = 0.032381
  Batch 20/29: Loss = 0.033905

Epoch 104/200 | Loss: 0.028889 | LR: 0.000195 | Time: 0.9s

  Batch 0/29: Loss = 0.039562
  Batch 10/29: Loss = 0.042711
  Batch 20/29: Loss = 0.021248

Epoch 105/200 | Loss: 0.028400 | LR: 0.000192 | Time: 1.0s

  ✓ Saved best model (loss=0.028400)
  Batch 0/29: Loss = 0.017528
  Batch 10/29: Loss = 0.031349
  Batch 20/29: Loss = 0.025664

Epoch 106/200 | Loss: 0.028223 | LR: 0.000189 | Time: 0.9s

  ✓ Saved best model (loss=0.028223)
  Batch 0/29: Loss = 0.032850
  Batch 10/29: Loss = 0.037328
  Batch 20/29: Loss = 0.043182

Epoch 107/200 | Loss: 0.032906 | LR: 0.000186 | Time: 1.0s

  Batch 0/29: Loss = 0.037444
  Batch 10/29: Loss = 0.019699
  Batch 20/29: Loss = 0.011206

Epoch 108/200 | Loss: 0.034045 | LR: 0.000182 | Time: 1.0s

  Batch 0/29: Loss = 0.024905
  Batch 10/29: Loss = 0.021491
  Batch 20/29: Loss = 0.016728

Epoch 109/200 | Loss: 0.029971 | LR: 0.000179 | Time: 0.8s

  Batch 0/29: Loss = 0.027147
  Batch 10/29: Loss = 0.006735
  Batch 20/29: Loss = 0.046342

Epoch 110/200 | Loss: 0.029569 | LR: 0.000176 | Time: 1.0s

  Batch 0/29: Loss = 0.040752
  Batch 10/29: Loss = 0.029156
  Batch 20/29: Loss = 0.032913

Epoch 111/200 | Loss: 0.029833 | LR: 0.000173 | Time: 1.0s

  Batch 0/29: Loss = 0.052412
  Batch 10/29: Loss = 0.035452
  Batch 20/29: Loss = 0.010134

Epoch 112/200 | Loss: 0.029059 | LR: 0.000170 | Time: 1.0s

  Batch 0/29: Loss = 0.061827
  Batch 10/29: Loss = 0.050050
  Batch 20/29: Loss = 0.051421

Epoch 113/200 | Loss: 0.030865 | LR: 0.000166 | Time: 0.8s

  Batch 0/29: Loss = 0.047427
  Batch 10/29: Loss = 0.013890
  Batch 20/29: Loss = 0.030596

Epoch 114/200 | Loss: 0.032220 | LR: 0.000163 | Time: 0.8s

  Batch 0/29: Loss = 0.036272
  Batch 10/29: Loss = 0.019768
  Batch 20/29: Loss = 0.054262

Epoch 115/200 | Loss: 0.027732 | LR: 0.000160 | Time: 1.1s

  ✓ Saved best model (loss=0.027732)
  Batch 0/29: Loss = 0.029988
  Batch 10/29: Loss = 0.022467
  Batch 20/29: Loss = 0.068310

Epoch 116/200 | Loss: 0.032720 | LR: 0.000157 | Time: 1.0s

  Batch 0/29: Loss = 0.020879
  Batch 10/29: Loss = 0.012712
  Batch 20/29: Loss = 0.069503

Epoch 117/200 | Loss: 0.029454 | LR: 0.000154 | Time: 1.0s

  Batch 0/29: Loss = 0.030657
  Batch 10/29: Loss = 0.019227
  Batch 20/29: Loss = 0.019720

Epoch 118/200 | Loss: 0.035487 | LR: 0.000151 | Time: 1.1s

  Batch 0/29: Loss = 0.020322
  Batch 10/29: Loss = 0.026803
  Batch 20/29: Loss = 0.028572

Epoch 119/200 | Loss: 0.029554 | LR: 0.000147 | Time: 1.0s

  Batch 0/29: Loss = 0.067294
  Batch 10/29: Loss = 0.008018
  Batch 20/29: Loss = 0.044345

Epoch 120/200 | Loss: 0.035798 | LR: 0.000144 | Time: 1.0s

  Batch 0/29: Loss = 0.027980
  Batch 10/29: Loss = 0.014150
  Batch 20/29: Loss = 0.038360

Epoch 121/200 | Loss: 0.031801 | LR: 0.000141 | Time: 0.9s

  Batch 0/29: Loss = 0.017988
  Batch 10/29: Loss = 0.020575
  Batch 20/29: Loss = 0.011777

Epoch 122/200 | Loss: 0.030417 | LR: 0.000138 | Time: 1.0s

  Batch 0/29: Loss = 0.009997
  Batch 10/29: Loss = 0.020821
  Batch 20/29: Loss = 0.045320

Epoch 123/200 | Loss: 0.030016 | LR: 0.000135 | Time: 0.9s

  Batch 0/29: Loss = 0.003728
  Batch 10/29: Loss = 0.031184
  Batch 20/29: Loss = 0.029352

Epoch 124/200 | Loss: 0.024274 | LR: 0.000132 | Time: 0.9s

  ✓ Saved best model (loss=0.024274)
  Batch 0/29: Loss = 0.036639
  Batch 10/29: Loss = 0.035318
  Batch 20/29: Loss = 0.006297

Epoch 125/200 | Loss: 0.032332 | LR: 0.000129 | Time: 0.9s

  Batch 0/29: Loss = 0.052667
  Batch 10/29: Loss = 0.023097
  Batch 20/29: Loss = 0.018470

Epoch 126/200 | Loss: 0.035274 | LR: 0.000126 | Time: 1.0s

  Batch 0/29: Loss = 0.030174
  Batch 10/29: Loss = 0.010283
  Batch 20/29: Loss = 0.034054

Epoch 127/200 | Loss: 0.030525 | LR: 0.000123 | Time: 1.7s

  Batch 0/29: Loss = 0.036069
  Batch 10/29: Loss = 0.055955
  Batch 20/29: Loss = 0.021522

Epoch 128/200 | Loss: 0.030152 | LR: 0.000120 | Time: 0.8s

  Batch 0/29: Loss = 0.012546
  Batch 10/29: Loss = 0.039713
  Batch 20/29: Loss = 0.010586

Epoch 129/200 | Loss: 0.028138 | LR: 0.000117 | Time: 1.0s

  Batch 0/29: Loss = 0.019445
  Batch 10/29: Loss = 0.029494
  Batch 20/29: Loss = 0.046520

Epoch 130/200 | Loss: 0.031929 | LR: 0.000114 | Time: 1.0s

  Batch 0/29: Loss = 0.074132
  Batch 10/29: Loss = 0.042524
  Batch 20/29: Loss = 0.024551

Epoch 131/200 | Loss: 0.029109 | LR: 0.000111 | Time: 0.8s

  Batch 0/29: Loss = 0.030811
  Batch 10/29: Loss = 0.034278
  Batch 20/29: Loss = 0.023960

Epoch 132/200 | Loss: 0.031140 | LR: 0.000108 | Time: 0.8s

  Batch 0/29: Loss = 0.025670
  Batch 10/29: Loss = 0.011321
  Batch 20/29: Loss = 0.037145

Epoch 133/200 | Loss: 0.026373 | LR: 0.000106 | Time: 1.0s

  Batch 0/29: Loss = 0.025317
  Batch 10/29: Loss = 0.021661
  Batch 20/29: Loss = 0.009245

Epoch 134/200 | Loss: 0.033095 | LR: 0.000103 | Time: 1.0s

  Batch 0/29: Loss = 0.039439
  Batch 10/29: Loss = 0.040126
  Batch 20/29: Loss = 0.032960

Epoch 135/200 | Loss: 0.033004 | LR: 0.000100 | Time: 1.0s

  Batch 0/29: Loss = 0.009868
  Batch 10/29: Loss = 0.012674
  Batch 20/29: Loss = 0.034050

Epoch 136/200 | Loss: 0.027559 | LR: 0.000097 | Time: 0.8s

  Batch 0/29: Loss = 0.020832
  Batch 10/29: Loss = 0.031559
  Batch 20/29: Loss = 0.028940

Epoch 137/200 | Loss: 0.027620 | LR: 0.000094 | Time: 1.0s

  Batch 0/29: Loss = 0.008095
  Batch 10/29: Loss = 0.009708
  Batch 20/29: Loss = 0.017340

Epoch 138/200 | Loss: 0.027310 | LR: 0.000092 | Time: 0.9s

  Batch 0/29: Loss = 0.059642
  Batch 10/29: Loss = 0.022032
  Batch 20/29: Loss = 0.023206

Epoch 139/200 | Loss: 0.030401 | LR: 0.000089 | Time: 0.9s

  Batch 0/29: Loss = 0.034839
  Batch 10/29: Loss = 0.022807
  Batch 20/29: Loss = 0.034001

Epoch 140/200 | Loss: 0.030088 | LR: 0.000086 | Time: 0.9s

  Batch 0/29: Loss = 0.005377
  Batch 10/29: Loss = 0.032252
  Batch 20/29: Loss = 0.023282

Epoch 141/200 | Loss: 0.028694 | LR: 0.000084 | Time: 1.0s

  Batch 0/29: Loss = 0.026578
  Batch 10/29: Loss = 0.023645
  Batch 20/29: Loss = 0.032998

Epoch 142/200 | Loss: 0.029397 | LR: 0.000081 | Time: 1.0s

  Batch 0/29: Loss = 0.022865
  Batch 10/29: Loss = 0.037347
  Batch 20/29: Loss = 0.010605

Epoch 143/200 | Loss: 0.028943 | LR: 0.000079 | Time: 0.9s

  Batch 0/29: Loss = 0.015880
  Batch 10/29: Loss = 0.018833
  Batch 20/29: Loss = 0.007563

Epoch 144/200 | Loss: 0.031658 | LR: 0.000076 | Time: 0.8s

  Batch 0/29: Loss = 0.030720
  Batch 10/29: Loss = 0.028170
  Batch 20/29: Loss = 0.013486

Epoch 145/200 | Loss: 0.033491 | LR: 0.000074 | Time: 1.0s

  Batch 0/29: Loss = 0.013733
  Batch 10/29: Loss = 0.021348
  Batch 20/29: Loss = 0.016004

Epoch 146/200 | Loss: 0.028707 | LR: 0.000071 | Time: 1.0s

  Batch 0/29: Loss = 0.024459
  Batch 10/29: Loss = 0.055935
  Batch 20/29: Loss = 0.059858

Epoch 147/200 | Loss: 0.027727 | LR: 0.000069 | Time: 0.9s

  Batch 0/29: Loss = 0.022529
  Batch 10/29: Loss = 0.045052
  Batch 20/29: Loss = 0.029540

Epoch 148/200 | Loss: 0.029804 | LR: 0.000066 | Time: 1.0s

  Batch 0/29: Loss = 0.058872
  Batch 10/29: Loss = 0.054147
  Batch 20/29: Loss = 0.038627

Epoch 149/200 | Loss: 0.027515 | LR: 0.000064 | Time: 0.9s

  Batch 0/29: Loss = 0.016138
  Batch 10/29: Loss = 0.010073
  Batch 20/29: Loss = 0.057157

Epoch 150/200 | Loss: 0.028108 | LR: 0.000061 | Time: 0.9s

  Batch 0/29: Loss = 0.032742
  Batch 10/29: Loss = 0.018911
  Batch 20/29: Loss = 0.052359

Epoch 151/200 | Loss: 0.027524 | LR: 0.000059 | Time: 0.9s

  Batch 0/29: Loss = 0.033020
  Batch 10/29: Loss = 0.021812
  Batch 20/29: Loss = 0.039733

Epoch 152/200 | Loss: 0.030689 | LR: 0.000057 | Time: 0.9s

  Batch 0/29: Loss = 0.011306
  Batch 10/29: Loss = 0.059585
  Batch 20/29: Loss = 0.029336

Epoch 153/200 | Loss: 0.026878 | LR: 0.000055 | Time: 1.0s

  Batch 0/29: Loss = 0.009016
  Batch 10/29: Loss = 0.025548
  Batch 20/29: Loss = 0.023239

Epoch 154/200 | Loss: 0.029110 | LR: 0.000052 | Time: 1.0s

  Batch 0/29: Loss = 0.022832
  Batch 10/29: Loss = 0.049511
  Batch 20/29: Loss = 0.024205

Epoch 155/200 | Loss: 0.026329 | LR: 0.000050 | Time: 1.0s

  Batch 0/29: Loss = 0.018968
  Batch 10/29: Loss = 0.014143
  Batch 20/29: Loss = 0.062586

Epoch 156/200 | Loss: 0.028939 | LR: 0.000048 | Time: 1.0s

  Batch 0/29: Loss = 0.020795
  Batch 10/29: Loss = 0.023887
  Batch 20/29: Loss = 0.063781

Epoch 157/200 | Loss: 0.028875 | LR: 0.000046 | Time: 1.0s

  Batch 0/29: Loss = 0.027305
  Batch 10/29: Loss = 0.034622
  Batch 20/29: Loss = 0.023880

Epoch 158/200 | Loss: 0.026295 | LR: 0.000044 | Time: 1.0s

  Batch 0/29: Loss = 0.009230
  Batch 10/29: Loss = 0.025427
  Batch 20/29: Loss = 0.021234

Epoch 159/200 | Loss: 0.025876 | LR: 0.000042 | Time: 0.9s

  Batch 0/29: Loss = 0.045450
  Batch 10/29: Loss = 0.047633
  Batch 20/29: Loss = 0.033864

Epoch 160/200 | Loss: 0.025218 | LR: 0.000040 | Time: 1.0s

  Batch 0/29: Loss = 0.057982
  Batch 10/29: Loss = 0.022414
  Batch 20/29: Loss = 0.014133

Epoch 161/200 | Loss: 0.024826 | LR: 0.000038 | Time: 1.8s

  Batch 0/29: Loss = 0.022970
  Batch 10/29: Loss = 0.002194
  Batch 20/29: Loss = 0.004621

Epoch 162/200 | Loss: 0.024909 | LR: 0.000036 | Time: 1.0s

  Batch 0/29: Loss = 0.015779
  Batch 10/29: Loss = 0.020519
  Batch 20/29: Loss = 0.015552

Epoch 163/200 | Loss: 0.027252 | LR: 0.000034 | Time: 1.0s

  Batch 0/29: Loss = 0.019389
  Batch 10/29: Loss = 0.012069
  Batch 20/29: Loss = 0.048198

Epoch 164/200 | Loss: 0.027898 | LR: 0.000033 | Time: 1.0s

  Batch 0/29: Loss = 0.019445
  Batch 10/29: Loss = 0.013496
  Batch 20/29: Loss = 0.017215

Epoch 165/200 | Loss: 0.024878 | LR: 0.000031 | Time: 1.0s

  Batch 0/29: Loss = 0.027502
  Batch 10/29: Loss = 0.016527
  Batch 20/29: Loss = 0.018571

Epoch 166/200 | Loss: 0.024190 | LR: 0.000029 | Time: 1.0s

  ✓ Saved best model (loss=0.024190)
  Batch 0/29: Loss = 0.009104
  Batch 10/29: Loss = 0.065423
  Batch 20/29: Loss = 0.014440

Epoch 167/200 | Loss: 0.023820 | LR: 0.000028 | Time: 1.0s

  ✓ Saved best model (loss=0.023820)
  Batch 0/29: Loss = 0.044333
  Batch 10/29: Loss = 0.015758
  Batch 20/29: Loss = 0.016955

Epoch 168/200 | Loss: 0.023271 | LR: 0.000026 | Time: 1.0s

  ✓ Saved best model (loss=0.023271)
  Batch 0/29: Loss = 0.022958
  Batch 10/29: Loss = 0.018486
  Batch 20/29: Loss = 0.055472

Epoch 169/200 | Loss: 0.025839 | LR: 0.000024 | Time: 1.0s

  Batch 0/29: Loss = 0.019993
  Batch 10/29: Loss = 0.018126
  Batch 20/29: Loss = 0.011140

Epoch 170/200 | Loss: 0.024162 | LR: 0.000023 | Time: 1.0s

  Batch 0/29: Loss = 0.018784
  Batch 10/29: Loss = 0.031634
  Batch 20/29: Loss = 0.037678

Epoch 171/200 | Loss: 0.025308 | LR: 0.000021 | Time: 0.9s

  Batch 0/29: Loss = 0.011796
  Batch 10/29: Loss = 0.015801
  Batch 20/29: Loss = 0.037128

Epoch 172/200 | Loss: 0.028234 | LR: 0.000020 | Time: 1.1s

  Batch 0/29: Loss = 0.057326
  Batch 10/29: Loss = 0.019551
  Batch 20/29: Loss = 0.033666

Epoch 173/200 | Loss: 0.024487 | LR: 0.000019 | Time: 0.9s

  Batch 0/29: Loss = 0.017339
  Batch 10/29: Loss = 0.040088
  Batch 20/29: Loss = 0.016958

Epoch 174/200 | Loss: 0.022786 | LR: 0.000017 | Time: 0.9s

  ✓ Saved best model (loss=0.022786)
  Batch 0/29: Loss = 0.018454
  Batch 10/29: Loss = 0.049297
  Batch 20/29: Loss = 0.018005

Epoch 175/200 | Loss: 0.025551 | LR: 0.000016 | Time: 0.8s

  Batch 0/29: Loss = 0.048829
  Batch 10/29: Loss = 0.030960
  Batch 20/29: Loss = 0.038875

Epoch 176/200 | Loss: 0.030588 | LR: 0.000015 | Time: 0.7s

  Batch 0/29: Loss = 0.025809
  Batch 10/29: Loss = 0.003698
  Batch 20/29: Loss = 0.017923

Epoch 177/200 | Loss: 0.026276 | LR: 0.000014 | Time: 0.7s

  Batch 0/29: Loss = 0.055200
  Batch 10/29: Loss = 0.014223
  Batch 20/29: Loss = 0.004042

Epoch 178/200 | Loss: 0.029115 | LR: 0.000012 | Time: 0.7s

  Batch 0/29: Loss = 0.020504
  Batch 10/29: Loss = 0.051900
  Batch 20/29: Loss = 0.014503

Epoch 179/200 | Loss: 0.024688 | LR: 0.000011 | Time: 0.7s

  Batch 0/29: Loss = 0.024156
  Batch 10/29: Loss = 0.008625
  Batch 20/29: Loss = 0.036702

Epoch 180/200 | Loss: 0.023404 | LR: 0.000010 | Time: 0.7s

  Batch 0/29: Loss = 0.008784
  Batch 10/29: Loss = 0.015224
  Batch 20/29: Loss = 0.032209

Epoch 181/200 | Loss: 0.027557 | LR: 0.000009 | Time: 0.7s

  Batch 0/29: Loss = 0.035246
  Batch 10/29: Loss = 0.012889
  Batch 20/29: Loss = 0.020885

Epoch 182/200 | Loss: 0.023407 | LR: 0.000008 | Time: 0.7s

  Batch 0/29: Loss = 0.025594
  Batch 10/29: Loss = 0.038157
  Batch 20/29: Loss = 0.010828

Epoch 183/200 | Loss: 0.028291 | LR: 0.000007 | Time: 0.8s

  Batch 0/29: Loss = 0.012876
  Batch 10/29: Loss = 0.063538
  Batch 20/29: Loss = 0.024186

Epoch 184/200 | Loss: 0.029955 | LR: 0.000007 | Time: 0.7s

  Batch 0/29: Loss = 0.012213
  Batch 10/29: Loss = 0.016983
  Batch 20/29: Loss = 0.022011

Epoch 185/200 | Loss: 0.024865 | LR: 0.000006 | Time: 0.7s

  Batch 0/29: Loss = 0.022586
  Batch 10/29: Loss = 0.039649
  Batch 20/29: Loss = 0.041738

Epoch 186/200 | Loss: 0.024029 | LR: 0.000005 | Time: 0.7s

  Batch 0/29: Loss = 0.019027
  Batch 10/29: Loss = 0.044602
  Batch 20/29: Loss = 0.021815

Epoch 187/200 | Loss: 0.028701 | LR: 0.000004 | Time: 0.8s

  Batch 0/29: Loss = 0.026593
  Batch 10/29: Loss = 0.019273
  Batch 20/29: Loss = 0.013486

Epoch 188/200 | Loss: 0.025120 | LR: 0.000004 | Time: 0.7s

  Batch 0/29: Loss = 0.038940
  Batch 10/29: Loss = 0.044603
  Batch 20/29: Loss = 0.025296

Epoch 189/200 | Loss: 0.026908 | LR: 0.000003 | Time: 0.7s

  Batch 0/29: Loss = 0.012489
  Batch 10/29: Loss = 0.017039
  Batch 20/29: Loss = 0.015247

Epoch 190/200 | Loss: 0.028056 | LR: 0.000003 | Time: 0.6s

  Batch 0/29: Loss = 0.021340
  Batch 10/29: Loss = 0.036023
  Batch 20/29: Loss = 0.029943

Epoch 191/200 | Loss: 0.024968 | LR: 0.000002 | Time: 0.7s

  Batch 0/29: Loss = 0.009950
  Batch 10/29: Loss = 0.029694
  Batch 20/29: Loss = 0.022907

Epoch 192/200 | Loss: 0.028563 | LR: 0.000002 | Time: 0.7s

  Batch 0/29: Loss = 0.029359
  Batch 10/29: Loss = 0.041126
  Batch 20/29: Loss = 0.022787

Epoch 193/200 | Loss: 0.026940 | LR: 0.000001 | Time: 0.7s

  Batch 0/29: Loss = 0.030659
  Batch 10/29: Loss = 0.051780
  Batch 20/29: Loss = 0.037234

Epoch 194/200 | Loss: 0.026728 | LR: 0.000001 | Time: 0.6s

  Batch 0/29: Loss = 0.012158
  Batch 10/29: Loss = 0.024065
  Batch 20/29: Loss = 0.024330

Epoch 195/200 | Loss: 0.025901 | LR: 0.000001 | Time: 0.4s

  Batch 0/29: Loss = 0.008443
  Batch 10/29: Loss = 0.030932
  Batch 20/29: Loss = 0.028107

Epoch 196/200 | Loss: 0.026651 | LR: 0.000000 | Time: 0.4s

  Batch 0/29: Loss = -0.003546
  Batch 10/29: Loss = 0.028479
  Batch 20/29: Loss = 0.035348

Epoch 197/200 | Loss: 0.021885 | LR: 0.000000 | Time: 0.4s

  ✓ Saved best model (loss=0.021885)
  Batch 0/29: Loss = 0.050729
  Batch 10/29: Loss = 0.005060
  Batch 20/29: Loss = 0.025854

Epoch 198/200 | Loss: 0.024615 | LR: 0.000000 | Time: 1.1s

  Batch 0/29: Loss = 0.011186
  Batch 10/29: Loss = 0.024073
  Batch 20/29: Loss = 0.014989

Epoch 199/200 | Loss: 0.023294 | LR: 0.000000 | Time: 0.4s

  Batch 0/29: Loss = 0.013676
  Batch 10/29: Loss = 0.011875
  Batch 20/29: Loss = 0.034743

Epoch 200/200 | Loss: 0.025543 | LR: 0.000000 | Time: 0.4s


============================================================
Training completed!
Best loss: 0.021885
============================================================

4. Evaluating...
/home/menserve/Object-centric-representation/src/train_movi.py:401: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()
/home/menserve/Object-centric-representation/src/train_movi.py:434: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()
Test Loss: 0.036011

5. Visualizing results...
Saved to checkpoints/clip_normalized/clip_vitb16/movi_result.png
Saved to checkpoints/clip_normalized/clip_vitb16/training_history.png

✅ Training completed!
