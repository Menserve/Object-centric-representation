nohup: ignoring input
Using cache found in /home/menserve/.cache/torch/hub/facebookresearch_dino_main
Device: cuda
Backbone: dino_vits16

1. Loading MOVi-A dataset...
MoviDataset: Found 60 samples (split=all)
Train samples: 58
Test samples: 2

2. Creating SAVi-DINOSAUR model with dino_vits16...
Loading dino_vits16 model...
Mask temperature (τ): 0.5
Trainable parameters: 9,026,369

3. Training...

============================================================
Training SAVi-DINOSAUR on MOVi-A
============================================================
Device: cuda
Epochs: 200
Learning rate: 0.0004 (with 5-epoch warmup)
Batch size: 2
Dataset size: 58
Diversity weight: 0.1
============================================================

  Batch 0/29: Loss = 1.056996
  Batch 10/29: Loss = 1.058534
  Batch 20/29: Loss = 1.050038

Epoch 1/200 | Loss: 1.054141 | LR: 0.000083 | Time: 1.2s

  ✓ Saved best model (loss=1.054141)
  Batch 0/29: Loss = 1.049764
  Batch 10/29: Loss = 0.908637
  Batch 20/29: Loss = 0.839873

Epoch 2/200 | Loss: 0.888380 | LR: 0.000162 | Time: 0.4s

  ✓ Saved best model (loss=0.888380)
  Batch 0/29: Loss = 0.792127
  Batch 10/29: Loss = 0.766756
  Batch 20/29: Loss = 0.774206

Epoch 3/200 | Loss: 0.784772 | LR: 0.000242 | Time: 0.4s

  ✓ Saved best model (loss=0.784772)
  Batch 0/29: Loss = 0.721007
  Batch 10/29: Loss = 0.824015
  Batch 20/29: Loss = 0.741099

Epoch 4/200 | Loss: 0.764340 | LR: 0.000321 | Time: 0.4s

  ✓ Saved best model (loss=0.764340)
  Batch 0/29: Loss = 0.751838
  Batch 10/29: Loss = 0.726995
  Batch 20/29: Loss = 0.686521

Epoch 5/200 | Loss: 0.722884 | LR: 0.000400 | Time: 0.5s

  ✓ Saved best model (loss=0.722884)
  Batch 0/29: Loss = 0.735058
  Batch 10/29: Loss = 0.699764
  Batch 20/29: Loss = 0.718692

Epoch 6/200 | Loss: 0.710016 | LR: 0.000400 | Time: 0.5s

  ✓ Saved best model (loss=0.710016)
  Batch 0/29: Loss = 0.728304
  Batch 10/29: Loss = 0.723091
  Batch 20/29: Loss = 0.696538

Epoch 7/200 | Loss: 0.695009 | LR: 0.000400 | Time: 0.5s

  ✓ Saved best model (loss=0.695009)
  Batch 0/29: Loss = 0.676738
  Batch 10/29: Loss = 0.673807
  Batch 20/29: Loss = 0.636216

Epoch 8/200 | Loss: 0.687626 | LR: 0.000400 | Time: 0.4s

  ✓ Saved best model (loss=0.687626)
  Batch 0/29: Loss = 0.625597
  Batch 10/29: Loss = 0.731873
  Batch 20/29: Loss = 0.689437

Epoch 9/200 | Loss: 0.684012 | LR: 0.000400 | Time: 0.5s

  ✓ Saved best model (loss=0.684012)
  Batch 0/29: Loss = 0.696065
  Batch 10/29: Loss = 0.675198
  Batch 20/29: Loss = 0.748496

Epoch 10/200 | Loss: 0.674412 | LR: 0.000399 | Time: 0.5s

  ✓ Saved best model (loss=0.674412)
  Batch 0/29: Loss = 0.753418
  Batch 10/29: Loss = 0.697080
  Batch 20/29: Loss = 0.662284

Epoch 11/200 | Loss: 0.673807 | LR: 0.000399 | Time: 0.5s

  ✓ Saved best model (loss=0.673807)
  Batch 0/29: Loss = 0.664531
  Batch 10/29: Loss = 0.634173
  Batch 20/29: Loss = 0.697741

Epoch 12/200 | Loss: 0.665141 | LR: 0.000399 | Time: 0.5s

  ✓ Saved best model (loss=0.665141)
  Batch 0/29: Loss = 0.625018
  Batch 10/29: Loss = 0.641086
  Batch 20/29: Loss = 0.646033

Epoch 13/200 | Loss: 0.657892 | LR: 0.000398 | Time: 0.6s

  ✓ Saved best model (loss=0.657892)
  Batch 0/29: Loss = 0.691340
  Batch 10/29: Loss = 0.665344
  Batch 20/29: Loss = 0.642750

Epoch 14/200 | Loss: 0.658380 | LR: 0.000398 | Time: 0.5s

  Batch 0/29: Loss = 0.698317
  Batch 10/29: Loss = 0.654873
  Batch 20/29: Loss = 0.655782

Epoch 15/200 | Loss: 0.654609 | LR: 0.000397 | Time: 0.6s

  ✓ Saved best model (loss=0.654609)
  Batch 0/29: Loss = 0.661648
  Batch 10/29: Loss = 0.647952
  Batch 20/29: Loss = 0.626583

Epoch 16/200 | Loss: 0.657665 | LR: 0.000397 | Time: 0.4s

  Batch 0/29: Loss = 0.573037
  Batch 10/29: Loss = 0.634329
  Batch 20/29: Loss = 0.730232

Epoch 17/200 | Loss: 0.657585 | LR: 0.000396 | Time: 0.5s

  Batch 0/29: Loss = 0.630565
  Batch 10/29: Loss = 0.724740
  Batch 20/29: Loss = 0.710656

Epoch 18/200 | Loss: 0.651698 | LR: 0.000396 | Time: 0.7s

  ✓ Saved best model (loss=0.651698)
  Batch 0/29: Loss = 0.609549
  Batch 10/29: Loss = 0.690395
  Batch 20/29: Loss = 0.626897

Epoch 19/200 | Loss: 0.646759 | LR: 0.000395 | Time: 0.6s

  ✓ Saved best model (loss=0.646759)
  Batch 0/29: Loss = 0.621256
  Batch 10/29: Loss = 0.617807
  Batch 20/29: Loss = 0.569094

Epoch 20/200 | Loss: 0.630646 | LR: 0.000394 | Time: 0.5s

  ✓ Saved best model (loss=0.630646)
  Batch 0/29: Loss = 0.610656
  Batch 10/29: Loss = 0.650931
  Batch 20/29: Loss = 0.632964

Epoch 21/200 | Loss: 0.633804 | LR: 0.000393 | Time: 0.7s

  Batch 0/29: Loss = 0.610416
  Batch 10/29: Loss = 0.602093
  Batch 20/29: Loss = 0.623933

Epoch 22/200 | Loss: 0.645270 | LR: 0.000393 | Time: 0.6s

  Batch 0/29: Loss = 0.557281
  Batch 10/29: Loss = 0.654177
  Batch 20/29: Loss = 0.574761

Epoch 23/200 | Loss: 0.634624 | LR: 0.000392 | Time: 0.7s

  Batch 0/29: Loss = 0.593542
  Batch 10/29: Loss = 0.574801
  Batch 20/29: Loss = 0.592054

Epoch 24/200 | Loss: 0.630739 | LR: 0.000391 | Time: 0.6s

  Batch 0/29: Loss = 0.604226
  Batch 10/29: Loss = 0.640097
  Batch 20/29: Loss = 0.616618

Epoch 25/200 | Loss: 0.629633 | LR: 0.000390 | Time: 0.6s

  ✓ Saved best model (loss=0.629633)
  Batch 0/29: Loss = 0.612826
  Batch 10/29: Loss = 0.597265
  Batch 20/29: Loss = 0.604107

Epoch 26/200 | Loss: 0.620611 | LR: 0.000389 | Time: 0.9s

  ✓ Saved best model (loss=0.620611)
  Batch 0/29: Loss = 0.643006
  Batch 10/29: Loss = 0.691964
  Batch 20/29: Loss = 0.609469

Epoch 27/200 | Loss: 0.619396 | LR: 0.000388 | Time: 0.8s

  ✓ Saved best model (loss=0.619396)
  Batch 0/29: Loss = 0.579504
  Batch 10/29: Loss = 0.598212
  Batch 20/29: Loss = 0.667039

Epoch 28/200 | Loss: 0.614885 | LR: 0.000386 | Time: 0.6s

  ✓ Saved best model (loss=0.614885)
  Batch 0/29: Loss = 0.599088
  Batch 10/29: Loss = 0.677773
  Batch 20/29: Loss = 0.543373

Epoch 29/200 | Loss: 0.621403 | LR: 0.000385 | Time: 0.9s

  Batch 0/29: Loss = 0.567682
  Batch 10/29: Loss = 0.616945
  Batch 20/29: Loss = 0.576587

Epoch 30/200 | Loss: 0.614735 | LR: 0.000384 | Time: 0.5s

  ✓ Saved best model (loss=0.614735)
  Batch 0/29: Loss = 0.594207
  Batch 10/29: Loss = 0.533048
  Batch 20/29: Loss = 0.573770

Epoch 31/200 | Loss: 0.607436 | LR: 0.000383 | Time: 0.8s

  ✓ Saved best model (loss=0.607436)
  Batch 0/29: Loss = 0.576156
  Batch 10/29: Loss = 0.635932
  Batch 20/29: Loss = 0.610449

Epoch 32/200 | Loss: 0.600874 | LR: 0.000381 | Time: 0.7s

  ✓ Saved best model (loss=0.600874)
  Batch 0/29: Loss = 0.563070
  Batch 10/29: Loss = 0.595721
  Batch 20/29: Loss = 0.610258

Epoch 33/200 | Loss: 0.601804 | LR: 0.000380 | Time: 1.4s

  Batch 0/29: Loss = 0.619898
  Batch 10/29: Loss = 0.525658
  Batch 20/29: Loss = 0.691191

Epoch 34/200 | Loss: 0.595730 | LR: 0.000379 | Time: 0.7s

  ✓ Saved best model (loss=0.595730)
  Batch 0/29: Loss = 0.587596
  Batch 10/29: Loss = 0.641547
  Batch 20/29: Loss = 0.600642

Epoch 35/200 | Loss: 0.595586 | LR: 0.000377 | Time: 0.7s

  ✓ Saved best model (loss=0.595586)
  Batch 0/29: Loss = 0.571077
  Batch 10/29: Loss = 0.620032
  Batch 20/29: Loss = 0.671127

Epoch 36/200 | Loss: 0.592759 | LR: 0.000376 | Time: 1.0s

  ✓ Saved best model (loss=0.592759)
  Batch 0/29: Loss = 0.550143
  Batch 10/29: Loss = 0.591081
  Batch 20/29: Loss = 0.590781

Epoch 37/200 | Loss: 0.594516 | LR: 0.000374 | Time: 0.7s

  Batch 0/29: Loss = 0.667662
  Batch 10/29: Loss = 0.621949
  Batch 20/29: Loss = 0.590301

Epoch 38/200 | Loss: 0.599096 | LR: 0.000372 | Time: 0.9s

  Batch 0/29: Loss = 0.662868
  Batch 10/29: Loss = 0.592242
  Batch 20/29: Loss = 0.569197

Epoch 39/200 | Loss: 0.590466 | LR: 0.000371 | Time: 0.7s

  ✓ Saved best model (loss=0.590466)
  Batch 0/29: Loss = 0.565937
  Batch 10/29: Loss = 0.578177
  Batch 20/29: Loss = 0.633164

Epoch 40/200 | Loss: 0.590702 | LR: 0.000369 | Time: 1.0s

  Batch 0/29: Loss = 0.648024
  Batch 10/29: Loss = 0.513421
  Batch 20/29: Loss = 0.654933

Epoch 41/200 | Loss: 0.578987 | LR: 0.000367 | Time: 0.8s

  ✓ Saved best model (loss=0.578987)
  Batch 0/29: Loss = 0.469879
  Batch 10/29: Loss = 0.627448
  Batch 20/29: Loss = 0.525178

Epoch 42/200 | Loss: 0.574061 | LR: 0.000366 | Time: 1.0s

  ✓ Saved best model (loss=0.574061)
  Batch 0/29: Loss = 0.611673
  Batch 10/29: Loss = 0.625857
  Batch 20/29: Loss = 0.551918

Epoch 43/200 | Loss: 0.581935 | LR: 0.000364 | Time: 0.7s

  Batch 0/29: Loss = 0.578036
  Batch 10/29: Loss = 0.518866
  Batch 20/29: Loss = 0.652961

Epoch 44/200 | Loss: 0.575423 | LR: 0.000362 | Time: 1.0s

  Batch 0/29: Loss = 0.516859
  Batch 10/29: Loss = 0.554588
  Batch 20/29: Loss = 0.497435

Epoch 45/200 | Loss: 0.571922 | LR: 0.000360 | Time: 0.9s

  ✓ Saved best model (loss=0.571922)
  Batch 0/29: Loss = 0.622662
  Batch 10/29: Loss = 0.683801
  Batch 20/29: Loss = 0.567059

Epoch 46/200 | Loss: 0.578676 | LR: 0.000358 | Time: 0.8s

  Batch 0/29: Loss = 0.522176
  Batch 10/29: Loss = 0.602184
  Batch 20/29: Loss = 0.534894

Epoch 47/200 | Loss: 0.567927 | LR: 0.000356 | Time: 0.7s

  ✓ Saved best model (loss=0.567927)
  Batch 0/29: Loss = 0.474922
  Batch 10/29: Loss = 0.584718
  Batch 20/29: Loss = 0.631537

Epoch 48/200 | Loss: 0.570596 | LR: 0.000354 | Time: 1.0s

  Batch 0/29: Loss = 0.560201
  Batch 10/29: Loss = 0.457623
  Batch 20/29: Loss = 0.518164

Epoch 49/200 | Loss: 0.561655 | LR: 0.000352 | Time: 0.8s

  ✓ Saved best model (loss=0.561655)
  Batch 0/29: Loss = 0.557871
  Batch 10/29: Loss = 0.635570
  Batch 20/29: Loss = 0.606952

Epoch 50/200 | Loss: 0.554651 | LR: 0.000350 | Time: 1.0s

  ✓ Saved best model (loss=0.554651)
  Batch 0/29: Loss = 0.481982
  Batch 10/29: Loss = 0.622414
  Batch 20/29: Loss = 0.524965

Epoch 51/200 | Loss: 0.557707 | LR: 0.000348 | Time: 0.7s

  Batch 0/29: Loss = 0.496531
  Batch 10/29: Loss = 0.592111
  Batch 20/29: Loss = 0.528370

Epoch 52/200 | Loss: 0.558784 | LR: 0.000345 | Time: 0.9s

  Batch 0/29: Loss = 0.562156
  Batch 10/29: Loss = 0.664862
  Batch 20/29: Loss = 0.509241

Epoch 53/200 | Loss: 0.548982 | LR: 0.000343 | Time: 0.8s

  ✓ Saved best model (loss=0.548982)
  Batch 0/29: Loss = 0.529027
  Batch 10/29: Loss = 0.481459
  Batch 20/29: Loss = 0.585421

Epoch 54/200 | Loss: 0.544217 | LR: 0.000341 | Time: 1.0s

  ✓ Saved best model (loss=0.544217)
  Batch 0/29: Loss = 0.530938
  Batch 10/29: Loss = 0.634539
  Batch 20/29: Loss = 0.534965

Epoch 55/200 | Loss: 0.557292 | LR: 0.000339 | Time: 1.0s

  Batch 0/29: Loss = 0.618965
  Batch 10/29: Loss = 0.538650
  Batch 20/29: Loss = 0.527374

Epoch 56/200 | Loss: 0.543778 | LR: 0.000336 | Time: 0.8s

  ✓ Saved best model (loss=0.543778)
  Batch 0/29: Loss = 0.537333
  Batch 10/29: Loss = 0.554470
  Batch 20/29: Loss = 0.503996

Epoch 57/200 | Loss: 0.540748 | LR: 0.000334 | Time: 0.8s

  ✓ Saved best model (loss=0.540748)
  Batch 0/29: Loss = 0.549981
  Batch 10/29: Loss = 0.495936
  Batch 20/29: Loss = 0.593517

Epoch 58/200 | Loss: 0.541245 | LR: 0.000331 | Time: 0.6s

  Batch 0/29: Loss = 0.563915
  Batch 10/29: Loss = 0.626763
  Batch 20/29: Loss = 0.484490

Epoch 59/200 | Loss: 0.540496 | LR: 0.000329 | Time: 0.9s

  ✓ Saved best model (loss=0.540496)
  Batch 0/29: Loss = 0.462461
  Batch 10/29: Loss = 0.552365
  Batch 20/29: Loss = 0.546856

Epoch 60/200 | Loss: 0.531813 | LR: 0.000326 | Time: 1.1s

  ✓ Saved best model (loss=0.531813)
  Batch 0/29: Loss = 0.532142
  Batch 10/29: Loss = 0.313512
  Batch 20/29: Loss = 0.479863

Epoch 61/200 | Loss: 0.526987 | LR: 0.000324 | Time: 0.9s

  ✓ Saved best model (loss=0.526987)
  Batch 0/29: Loss = 0.610976
  Batch 10/29: Loss = 0.430615
  Batch 20/29: Loss = 0.579997

Epoch 62/200 | Loss: 0.515947 | LR: 0.000321 | Time: 0.8s

  ✓ Saved best model (loss=0.515947)
  Batch 0/29: Loss = 0.548068
  Batch 10/29: Loss = 0.440069
  Batch 20/29: Loss = 0.472526

Epoch 63/200 | Loss: 0.527506 | LR: 0.000319 | Time: 0.9s

  Batch 0/29: Loss = 0.588602
  Batch 10/29: Loss = 0.525028
  Batch 20/29: Loss = 0.527346

Epoch 64/200 | Loss: 0.524660 | LR: 0.000316 | Time: 0.8s

  Batch 0/29: Loss = 0.540246
  Batch 10/29: Loss = 0.553777
  Batch 20/29: Loss = 0.591295

Epoch 65/200 | Loss: 0.517998 | LR: 0.000314 | Time: 1.0s

  Batch 0/29: Loss = 0.428754
  Batch 10/29: Loss = 0.533309
  Batch 20/29: Loss = 0.618166

Epoch 66/200 | Loss: 0.508385 | LR: 0.000311 | Time: 0.9s

  ✓ Saved best model (loss=0.508385)
  Batch 0/29: Loss = 0.436903
  Batch 10/29: Loss = 0.425778
  Batch 20/29: Loss = 0.566397

Epoch 67/200 | Loss: 0.504311 | LR: 0.000308 | Time: 0.9s

  ✓ Saved best model (loss=0.504311)
  Batch 0/29: Loss = 0.559769
  Batch 10/29: Loss = 0.446853
  Batch 20/29: Loss = 0.488339

Epoch 68/200 | Loss: 0.495995 | LR: 0.000306 | Time: 1.0s

  ✓ Saved best model (loss=0.495995)
  Batch 0/29: Loss = 0.504202
  Batch 10/29: Loss = 0.564672
  Batch 20/29: Loss = 0.633925

Epoch 69/200 | Loss: 0.496772 | LR: 0.000303 | Time: 1.0s

  Batch 0/29: Loss = 0.508821
  Batch 10/29: Loss = 0.453354
  Batch 20/29: Loss = 0.540342

Epoch 70/200 | Loss: 0.501121 | LR: 0.000300 | Time: 1.0s

  Batch 0/29: Loss = 0.482294
  Batch 10/29: Loss = 0.557158
  Batch 20/29: Loss = 0.502880

Epoch 71/200 | Loss: 0.494179 | LR: 0.000297 | Time: 1.0s

  ✓ Saved best model (loss=0.494179)
  Batch 0/29: Loss = 0.371450
  Batch 10/29: Loss = 0.473043
  Batch 20/29: Loss = 0.584232

Epoch 72/200 | Loss: 0.489079 | LR: 0.000294 | Time: 0.7s

  ✓ Saved best model (loss=0.489079)
  Batch 0/29: Loss = 0.547595
  Batch 10/29: Loss = 0.552302
  Batch 20/29: Loss = 0.393552

Epoch 73/200 | Loss: 0.486931 | LR: 0.000292 | Time: 1.0s

  ✓ Saved best model (loss=0.486931)
  Batch 0/29: Loss = 0.512272
  Batch 10/29: Loss = 0.493554
  Batch 20/29: Loss = 0.478630

Epoch 74/200 | Loss: 0.485184 | LR: 0.000289 | Time: 0.9s

  ✓ Saved best model (loss=0.485184)
  Batch 0/29: Loss = 0.335701
  Batch 10/29: Loss = 0.485130
  Batch 20/29: Loss = 0.471453

Epoch 75/200 | Loss: 0.477806 | LR: 0.000286 | Time: 0.8s

  ✓ Saved best model (loss=0.477806)
  Batch 0/29: Loss = 0.557437
  Batch 10/29: Loss = 0.513942
  Batch 20/29: Loss = 0.449787

Epoch 76/200 | Loss: 0.477804 | LR: 0.000283 | Time: 1.1s

  ✓ Saved best model (loss=0.477804)
  Batch 0/29: Loss = 0.479384
  Batch 10/29: Loss = 0.470091
  Batch 20/29: Loss = 0.407467

Epoch 77/200 | Loss: 0.476074 | LR: 0.000280 | Time: 1.0s

  ✓ Saved best model (loss=0.476074)
  Batch 0/29: Loss = 0.493214
  Batch 10/29: Loss = 0.396997
  Batch 20/29: Loss = 0.432936

Epoch 78/200 | Loss: 0.466358 | LR: 0.000277 | Time: 0.9s

  ✓ Saved best model (loss=0.466358)
  Batch 0/29: Loss = 0.420645
  Batch 10/29: Loss = 0.488062
  Batch 20/29: Loss = 0.468907

Epoch 79/200 | Loss: 0.473879 | LR: 0.000274 | Time: 0.9s

  Batch 0/29: Loss = 0.494792
  Batch 10/29: Loss = 0.404449
  Batch 20/29: Loss = 0.397750

Epoch 80/200 | Loss: 0.463946 | LR: 0.000271 | Time: 0.9s

  ✓ Saved best model (loss=0.463946)
  Batch 0/29: Loss = 0.365613
  Batch 10/29: Loss = 0.486772
  Batch 20/29: Loss = 0.459605

Epoch 81/200 | Loss: 0.457043 | LR: 0.000268 | Time: 1.1s

  ✓ Saved best model (loss=0.457043)
  Batch 0/29: Loss = 0.481354
  Batch 10/29: Loss = 0.567019
  Batch 20/29: Loss = 0.443174

Epoch 82/200 | Loss: 0.455489 | LR: 0.000265 | Time: 0.9s

  ✓ Saved best model (loss=0.455489)
  Batch 0/29: Loss = 0.439825
  Batch 10/29: Loss = 0.358874
  Batch 20/29: Loss = 0.503005

Epoch 83/200 | Loss: 0.449758 | LR: 0.000262 | Time: 0.8s

  ✓ Saved best model (loss=0.449758)
  Batch 0/29: Loss = 0.404476
  Batch 10/29: Loss = 0.427729
  Batch 20/29: Loss = 0.401175

Epoch 84/200 | Loss: 0.447975 | LR: 0.000259 | Time: 1.1s

  ✓ Saved best model (loss=0.447975)
  Batch 0/29: Loss = 0.386322
  Batch 10/29: Loss = 0.451836
  Batch 20/29: Loss = 0.412855

Epoch 85/200 | Loss: 0.451537 | LR: 0.000256 | Time: 1.1s

  Batch 0/29: Loss = 0.493420
  Batch 10/29: Loss = 0.429854
  Batch 20/29: Loss = 0.381217

Epoch 86/200 | Loss: 0.450076 | LR: 0.000253 | Time: 1.1s

  Batch 0/29: Loss = 0.376418
  Batch 10/29: Loss = 0.381470
  Batch 20/29: Loss = 0.518535

Epoch 87/200 | Loss: 0.440158 | LR: 0.000249 | Time: 1.1s

  ✓ Saved best model (loss=0.440158)
  Batch 0/29: Loss = 0.504644
  Batch 10/29: Loss = 0.346650
  Batch 20/29: Loss = 0.449142

Epoch 88/200 | Loss: 0.438981 | LR: 0.000246 | Time: 1.2s

  ✓ Saved best model (loss=0.438981)
  Batch 0/29: Loss = 0.411108
  Batch 10/29: Loss = 0.497186
  Batch 20/29: Loss = 0.330348

Epoch 89/200 | Loss: 0.424443 | LR: 0.000243 | Time: 0.5s

  ✓ Saved best model (loss=0.424443)
  Batch 0/29: Loss = 0.427247
  Batch 10/29: Loss = 0.306586
  Batch 20/29: Loss = 0.553129

Epoch 90/200 | Loss: 0.438475 | LR: 0.000240 | Time: 1.1s

  Batch 0/29: Loss = 0.378353
  Batch 10/29: Loss = 0.352327
  Batch 20/29: Loss = 0.387726

Epoch 91/200 | Loss: 0.415456 | LR: 0.000237 | Time: 1.0s

  ✓ Saved best model (loss=0.415456)
  Batch 0/29: Loss = 0.350788
  Batch 10/29: Loss = 0.309954
  Batch 20/29: Loss = 0.519214

Epoch 92/200 | Loss: 0.414948 | LR: 0.000234 | Time: 1.0s

  ✓ Saved best model (loss=0.414948)
  Batch 0/29: Loss = 0.361858
  Batch 10/29: Loss = 0.477666
  Batch 20/29: Loss = 0.447814

Epoch 93/200 | Loss: 0.413198 | LR: 0.000230 | Time: 0.7s

  ✓ Saved best model (loss=0.413198)
  Batch 0/29: Loss = 0.487440
  Batch 10/29: Loss = 0.416248
  Batch 20/29: Loss = 0.391052

Epoch 94/200 | Loss: 0.407064 | LR: 0.000227 | Time: 1.1s

  ✓ Saved best model (loss=0.407064)
  Batch 0/29: Loss = 0.439505
  Batch 10/29: Loss = 0.425836
  Batch 20/29: Loss = 0.389218

Epoch 95/200 | Loss: 0.407034 | LR: 0.000224 | Time: 1.0s

  ✓ Saved best model (loss=0.407034)
  Batch 0/29: Loss = 0.428510
  Batch 10/29: Loss = 0.404210
  Batch 20/29: Loss = 0.390323

Epoch 96/200 | Loss: 0.407880 | LR: 0.000221 | Time: 1.0s

  Batch 0/29: Loss = 0.419070
  Batch 10/29: Loss = 0.358756
  Batch 20/29: Loss = 0.452963

Epoch 97/200 | Loss: 0.409348 | LR: 0.000218 | Time: 0.8s

  Batch 0/29: Loss = 0.332545
  Batch 10/29: Loss = 0.369970
  Batch 20/29: Loss = 0.403261

Epoch 98/200 | Loss: 0.403570 | LR: 0.000214 | Time: 1.0s

  ✓ Saved best model (loss=0.403570)
  Batch 0/29: Loss = 0.396536
  Batch 10/29: Loss = 0.414010
  Batch 20/29: Loss = 0.438231

Epoch 99/200 | Loss: 0.404097 | LR: 0.000211 | Time: 1.0s

  Batch 0/29: Loss = 0.386429
  Batch 10/29: Loss = 0.477097
  Batch 20/29: Loss = 0.386205

Epoch 100/200 | Loss: 0.399384 | LR: 0.000208 | Time: 1.1s

  ✓ Saved best model (loss=0.399384)
  Batch 0/29: Loss = 0.378348
  Batch 10/29: Loss = 0.450209
  Batch 20/29: Loss = 0.424408

Epoch 101/200 | Loss: 0.392902 | LR: 0.000205 | Time: 1.0s

  ✓ Saved best model (loss=0.392902)
  Batch 0/29: Loss = 0.369222
  Batch 10/29: Loss = 0.382253
  Batch 20/29: Loss = 0.417050

Epoch 102/200 | Loss: 0.385331 | LR: 0.000202 | Time: 1.0s

  ✓ Saved best model (loss=0.385331)
  Batch 0/29: Loss = 0.318655
  Batch 10/29: Loss = 0.338730
  Batch 20/29: Loss = 0.391648

Epoch 103/200 | Loss: 0.377682 | LR: 0.000198 | Time: 0.8s

  ✓ Saved best model (loss=0.377682)
  Batch 0/29: Loss = 0.429267
  Batch 10/29: Loss = 0.389586
  Batch 20/29: Loss = 0.378226

Epoch 104/200 | Loss: 0.379807 | LR: 0.000195 | Time: 0.9s

  Batch 0/29: Loss = 0.343889
  Batch 10/29: Loss = 0.409968
  Batch 20/29: Loss = 0.431508

Epoch 105/200 | Loss: 0.374575 | LR: 0.000192 | Time: 1.0s

  ✓ Saved best model (loss=0.374575)
  Batch 0/29: Loss = 0.350639
  Batch 10/29: Loss = 0.389232
  Batch 20/29: Loss = 0.377353

Epoch 106/200 | Loss: 0.365598 | LR: 0.000189 | Time: 1.0s

  ✓ Saved best model (loss=0.365598)
  Batch 0/29: Loss = 0.377043
  Batch 10/29: Loss = 0.264378
  Batch 20/29: Loss = 0.320350

Epoch 107/200 | Loss: 0.365610 | LR: 0.000186 | Time: 1.0s

  Batch 0/29: Loss = 0.405660
  Batch 10/29: Loss = 0.438022
  Batch 20/29: Loss = 0.357042

Epoch 108/200 | Loss: 0.364437 | LR: 0.000182 | Time: 1.0s

  ✓ Saved best model (loss=0.364437)
  Batch 0/29: Loss = 0.381574
  Batch 10/29: Loss = 0.419840
  Batch 20/29: Loss = 0.355489

Epoch 109/200 | Loss: 0.355342 | LR: 0.000179 | Time: 1.0s

  ✓ Saved best model (loss=0.355342)
  Batch 0/29: Loss = 0.305891
  Batch 10/29: Loss = 0.367889
  Batch 20/29: Loss = 0.328667

Epoch 110/200 | Loss: 0.360534 | LR: 0.000176 | Time: 1.1s

  Batch 0/29: Loss = 0.380291
  Batch 10/29: Loss = 0.275961
  Batch 20/29: Loss = 0.357013

Epoch 111/200 | Loss: 0.357114 | LR: 0.000173 | Time: 1.0s

  Batch 0/29: Loss = 0.313203
  Batch 10/29: Loss = 0.375023
  Batch 20/29: Loss = 0.314910

Epoch 112/200 | Loss: 0.351596 | LR: 0.000170 | Time: 1.0s

  ✓ Saved best model (loss=0.351596)
  Batch 0/29: Loss = 0.355408
  Batch 10/29: Loss = 0.269406
  Batch 20/29: Loss = 0.453883

Epoch 113/200 | Loss: 0.349596 | LR: 0.000166 | Time: 1.0s

  ✓ Saved best model (loss=0.349596)
  Batch 0/29: Loss = 0.391969
  Batch 10/29: Loss = 0.356896
  Batch 20/29: Loss = 0.337779

Epoch 114/200 | Loss: 0.350511 | LR: 0.000163 | Time: 1.0s

  Batch 0/29: Loss = 0.260677
  Batch 10/29: Loss = 0.395762
  Batch 20/29: Loss = 0.347527

Epoch 115/200 | Loss: 0.348274 | LR: 0.000160 | Time: 1.0s

  ✓ Saved best model (loss=0.348274)
  Batch 0/29: Loss = 0.288372
  Batch 10/29: Loss = 0.354324
  Batch 20/29: Loss = 0.378273

Epoch 116/200 | Loss: 0.342204 | LR: 0.000157 | Time: 1.0s

  ✓ Saved best model (loss=0.342204)
  Batch 0/29: Loss = 0.400654
  Batch 10/29: Loss = 0.361714
  Batch 20/29: Loss = 0.355907

Epoch 117/200 | Loss: 0.339011 | LR: 0.000154 | Time: 1.0s

  ✓ Saved best model (loss=0.339011)
  Batch 0/29: Loss = 0.361403
  Batch 10/29: Loss = 0.252153
  Batch 20/29: Loss = 0.357518

Epoch 118/200 | Loss: 0.340231 | LR: 0.000151 | Time: 1.0s

  Batch 0/29: Loss = 0.236858
  Batch 10/29: Loss = 0.319163
  Batch 20/29: Loss = 0.365866

Epoch 119/200 | Loss: 0.338049 | LR: 0.000147 | Time: 0.7s

  ✓ Saved best model (loss=0.338049)
  Batch 0/29: Loss = 0.342687
  Batch 10/29: Loss = 0.389640
  Batch 20/29: Loss = 0.421158

Epoch 120/200 | Loss: 0.341683 | LR: 0.000144 | Time: 0.9s

  Batch 0/29: Loss = 0.312271
  Batch 10/29: Loss = 0.374076
  Batch 20/29: Loss = 0.342110

Epoch 121/200 | Loss: 0.331101 | LR: 0.000141 | Time: 1.1s

  ✓ Saved best model (loss=0.331101)
  Batch 0/29: Loss = 0.326466
  Batch 10/29: Loss = 0.356615
  Batch 20/29: Loss = 0.306151

Epoch 122/200 | Loss: 0.335241 | LR: 0.000138 | Time: 0.7s

  Batch 0/29: Loss = 0.399696
  Batch 10/29: Loss = 0.349002
  Batch 20/29: Loss = 0.350115

Epoch 123/200 | Loss: 0.340132 | LR: 0.000135 | Time: 1.0s

  Batch 0/29: Loss = 0.356235
  Batch 10/29: Loss = 0.272855
  Batch 20/29: Loss = 0.249577

Epoch 124/200 | Loss: 0.325596 | LR: 0.000132 | Time: 1.1s

  ✓ Saved best model (loss=0.325596)
  Batch 0/29: Loss = 0.363536
  Batch 10/29: Loss = 0.373272
  Batch 20/29: Loss = 0.264698

Epoch 125/200 | Loss: 0.326858 | LR: 0.000129 | Time: 1.8s

  Batch 0/29: Loss = 0.320614
  Batch 10/29: Loss = 0.349234
  Batch 20/29: Loss = 0.361135

Epoch 126/200 | Loss: 0.316747 | LR: 0.000126 | Time: 1.0s

  ✓ Saved best model (loss=0.316747)
  Batch 0/29: Loss = 0.278823
  Batch 10/29: Loss = 0.330132
  Batch 20/29: Loss = 0.254059

Epoch 127/200 | Loss: 0.315254 | LR: 0.000123 | Time: 1.0s

  ✓ Saved best model (loss=0.315254)
  Batch 0/29: Loss = 0.313620
  Batch 10/29: Loss = 0.269925
  Batch 20/29: Loss = 0.344347

Epoch 128/200 | Loss: 0.310918 | LR: 0.000120 | Time: 1.0s

  ✓ Saved best model (loss=0.310918)
  Batch 0/29: Loss = 0.299284
  Batch 10/29: Loss = 0.294726
  Batch 20/29: Loss = 0.313690

Epoch 129/200 | Loss: 0.315676 | LR: 0.000117 | Time: 1.0s

  Batch 0/29: Loss = 0.358214
  Batch 10/29: Loss = 0.357032
  Batch 20/29: Loss = 0.325885

Epoch 130/200 | Loss: 0.313469 | LR: 0.000114 | Time: 1.0s

  Batch 0/29: Loss = 0.290921
  Batch 10/29: Loss = 0.276781
  Batch 20/29: Loss = 0.306071

Epoch 131/200 | Loss: 0.308974 | LR: 0.000111 | Time: 1.1s

  ✓ Saved best model (loss=0.308974)
  Batch 0/29: Loss = 0.294346
  Batch 10/29: Loss = 0.331537
  Batch 20/29: Loss = 0.368205

Epoch 132/200 | Loss: 0.310975 | LR: 0.000108 | Time: 1.0s

  Batch 0/29: Loss = 0.273879
  Batch 10/29: Loss = 0.309710
  Batch 20/29: Loss = 0.294168

Epoch 133/200 | Loss: 0.308343 | LR: 0.000106 | Time: 0.9s

  ✓ Saved best model (loss=0.308343)
  Batch 0/29: Loss = 0.322165
  Batch 10/29: Loss = 0.334093
  Batch 20/29: Loss = 0.271912

Epoch 134/200 | Loss: 0.310777 | LR: 0.000103 | Time: 0.9s

  Batch 0/29: Loss = 0.305780
  Batch 10/29: Loss = 0.300370
  Batch 20/29: Loss = 0.329293

Epoch 135/200 | Loss: 0.301864 | LR: 0.000100 | Time: 0.7s

  ✓ Saved best model (loss=0.301864)
  Batch 0/29: Loss = 0.259509
  Batch 10/29: Loss = 0.281441
  Batch 20/29: Loss = 0.313691

Epoch 136/200 | Loss: 0.300791 | LR: 0.000097 | Time: 1.0s

  ✓ Saved best model (loss=0.300791)
  Batch 0/29: Loss = 0.317196
  Batch 10/29: Loss = 0.405707
  Batch 20/29: Loss = 0.251448

Epoch 137/200 | Loss: 0.292759 | LR: 0.000094 | Time: 1.0s

  ✓ Saved best model (loss=0.292759)
  Batch 0/29: Loss = 0.321443
  Batch 10/29: Loss = 0.296465
  Batch 20/29: Loss = 0.296602

Epoch 138/200 | Loss: 0.294336 | LR: 0.000092 | Time: 1.1s

  Batch 0/29: Loss = 0.334776
  Batch 10/29: Loss = 0.324887
  Batch 20/29: Loss = 0.270509

Epoch 139/200 | Loss: 0.291875 | LR: 0.000089 | Time: 1.1s

  ✓ Saved best model (loss=0.291875)
  Batch 0/29: Loss = 0.234486
  Batch 10/29: Loss = 0.279900
  Batch 20/29: Loss = 0.273184

Epoch 140/200 | Loss: 0.291113 | LR: 0.000086 | Time: 1.1s

  ✓ Saved best model (loss=0.291113)
  Batch 0/29: Loss = 0.313159
  Batch 10/29: Loss = 0.319162
  Batch 20/29: Loss = 0.290656

Epoch 141/200 | Loss: 0.287616 | LR: 0.000084 | Time: 1.0s

  ✓ Saved best model (loss=0.287616)
  Batch 0/29: Loss = 0.320506
  Batch 10/29: Loss = 0.202980
  Batch 20/29: Loss = 0.295504

Epoch 142/200 | Loss: 0.297031 | LR: 0.000081 | Time: 1.1s

  Batch 0/29: Loss = 0.300887
  Batch 10/29: Loss = 0.313657
  Batch 20/29: Loss = 0.347750

Epoch 143/200 | Loss: 0.289499 | LR: 0.000079 | Time: 0.6s

  Batch 0/29: Loss = 0.267807
  Batch 10/29: Loss = 0.249787
  Batch 20/29: Loss = 0.212732

Epoch 144/200 | Loss: 0.288851 | LR: 0.000076 | Time: 1.0s

  Batch 0/29: Loss = 0.288223
  Batch 10/29: Loss = 0.260555
  Batch 20/29: Loss = 0.242103

Epoch 145/200 | Loss: 0.287935 | LR: 0.000074 | Time: 1.0s

  Batch 0/29: Loss = 0.304077
  Batch 10/29: Loss = 0.287186
  Batch 20/29: Loss = 0.256314

Epoch 146/200 | Loss: 0.290437 | LR: 0.000071 | Time: 1.1s

  Batch 0/29: Loss = 0.347428
  Batch 10/29: Loss = 0.308090
  Batch 20/29: Loss = 0.197239

Epoch 147/200 | Loss: 0.288307 | LR: 0.000069 | Time: 1.1s

  Batch 0/29: Loss = 0.336050
  Batch 10/29: Loss = 0.261179
  Batch 20/29: Loss = 0.271745

Epoch 148/200 | Loss: 0.285892 | LR: 0.000066 | Time: 1.0s

  ✓ Saved best model (loss=0.285892)
  Batch 0/29: Loss = 0.267057
  Batch 10/29: Loss = 0.255006
  Batch 20/29: Loss = 0.270708

Epoch 149/200 | Loss: 0.287006 | LR: 0.000064 | Time: 1.1s

  Batch 0/29: Loss = 0.284400
  Batch 10/29: Loss = 0.300924
  Batch 20/29: Loss = 0.261906

Epoch 150/200 | Loss: 0.285492 | LR: 0.000061 | Time: 1.0s

  ✓ Saved best model (loss=0.285492)
  Batch 0/29: Loss = 0.306704
  Batch 10/29: Loss = 0.357549
  Batch 20/29: Loss = 0.328924

Epoch 151/200 | Loss: 0.285465 | LR: 0.000059 | Time: 1.0s

  ✓ Saved best model (loss=0.285465)
  Batch 0/29: Loss = 0.359977
  Batch 10/29: Loss = 0.294194
  Batch 20/29: Loss = 0.257845

Epoch 152/200 | Loss: 0.278264 | LR: 0.000057 | Time: 0.7s

  ✓ Saved best model (loss=0.278264)
  Batch 0/29: Loss = 0.261066
  Batch 10/29: Loss = 0.243068
  Batch 20/29: Loss = 0.337960

Epoch 153/200 | Loss: 0.284042 | LR: 0.000055 | Time: 1.0s

  Batch 0/29: Loss = 0.231075
  Batch 10/29: Loss = 0.212888
  Batch 20/29: Loss = 0.339939

Epoch 154/200 | Loss: 0.281884 | LR: 0.000052 | Time: 1.7s

  Batch 0/29: Loss = 0.353846
  Batch 10/29: Loss = 0.214801
  Batch 20/29: Loss = 0.317337

Epoch 155/200 | Loss: 0.277459 | LR: 0.000050 | Time: 1.0s

  ✓ Saved best model (loss=0.277459)
  Batch 0/29: Loss = 0.305308
  Batch 10/29: Loss = 0.289057
  Batch 20/29: Loss = 0.306989

Epoch 156/200 | Loss: 0.279128 | LR: 0.000048 | Time: 1.0s

  Batch 0/29: Loss = 0.266841
  Batch 10/29: Loss = 0.256334
  Batch 20/29: Loss = 0.271723

Epoch 157/200 | Loss: 0.273862 | LR: 0.000046 | Time: 1.1s

  ✓ Saved best model (loss=0.273862)
  Batch 0/29: Loss = 0.288698
  Batch 10/29: Loss = 0.310965
  Batch 20/29: Loss = 0.291694

Epoch 158/200 | Loss: 0.273764 | LR: 0.000044 | Time: 1.0s

  ✓ Saved best model (loss=0.273764)
  Batch 0/29: Loss = 0.369313
  Batch 10/29: Loss = 0.314454
  Batch 20/29: Loss = 0.311369

Epoch 159/200 | Loss: 0.271589 | LR: 0.000042 | Time: 1.1s

  ✓ Saved best model (loss=0.271589)
  Batch 0/29: Loss = 0.228293
  Batch 10/29: Loss = 0.285715
  Batch 20/29: Loss = 0.262128

Epoch 160/200 | Loss: 0.274455 | LR: 0.000040 | Time: 1.1s

  Batch 0/29: Loss = 0.249022
  Batch 10/29: Loss = 0.294367
  Batch 20/29: Loss = 0.294574

Epoch 161/200 | Loss: 0.271466 | LR: 0.000038 | Time: 1.1s

  ✓ Saved best model (loss=0.271466)
  Batch 0/29: Loss = 0.230911
  Batch 10/29: Loss = 0.287991
  Batch 20/29: Loss = 0.300806

Epoch 162/200 | Loss: 0.272367 | LR: 0.000036 | Time: 1.1s

  Batch 0/29: Loss = 0.330685
  Batch 10/29: Loss = 0.222046
  Batch 20/29: Loss = 0.276378

Epoch 163/200 | Loss: 0.270043 | LR: 0.000034 | Time: 1.1s

  ✓ Saved best model (loss=0.270043)
  Batch 0/29: Loss = 0.231526
  Batch 10/29: Loss = 0.279252
  Batch 20/29: Loss = 0.256193

Epoch 164/200 | Loss: 0.272623 | LR: 0.000033 | Time: 1.0s

  Batch 0/29: Loss = 0.253035
  Batch 10/29: Loss = 0.268583
  Batch 20/29: Loss = 0.256279

Epoch 165/200 | Loss: 0.271336 | LR: 0.000031 | Time: 1.1s

  Batch 0/29: Loss = 0.302517
  Batch 10/29: Loss = 0.283314
  Batch 20/29: Loss = 0.244228

Epoch 166/200 | Loss: 0.275318 | LR: 0.000029 | Time: 1.1s

  Batch 0/29: Loss = 0.233536
  Batch 10/29: Loss = 0.307744
  Batch 20/29: Loss = 0.176367

Epoch 167/200 | Loss: 0.270670 | LR: 0.000028 | Time: 1.0s

  Batch 0/29: Loss = 0.228402
  Batch 10/29: Loss = 0.291340
  Batch 20/29: Loss = 0.296675

Epoch 168/200 | Loss: 0.267530 | LR: 0.000026 | Time: 1.0s

  ✓ Saved best model (loss=0.267530)
  Batch 0/29: Loss = 0.285698
  Batch 10/29: Loss = 0.217884
  Batch 20/29: Loss = 0.303045

Epoch 169/200 | Loss: 0.267856 | LR: 0.000024 | Time: 1.0s

  Batch 0/29: Loss = 0.273117
  Batch 10/29: Loss = 0.295849
  Batch 20/29: Loss = 0.323855

Epoch 170/200 | Loss: 0.271436 | LR: 0.000023 | Time: 1.0s

  Batch 0/29: Loss = 0.288227
  Batch 10/29: Loss = 0.289448
  Batch 20/29: Loss = 0.291461

Epoch 171/200 | Loss: 0.270040 | LR: 0.000021 | Time: 1.0s

  Batch 0/29: Loss = 0.294716
  Batch 10/29: Loss = 0.246535
  Batch 20/29: Loss = 0.253891

Epoch 172/200 | Loss: 0.263946 | LR: 0.000020 | Time: 1.1s

  ✓ Saved best model (loss=0.263946)
  Batch 0/29: Loss = 0.270278
  Batch 10/29: Loss = 0.241610
  Batch 20/29: Loss = 0.280062

Epoch 173/200 | Loss: 0.261091 | LR: 0.000019 | Time: 1.0s

  ✓ Saved best model (loss=0.261091)
  Batch 0/29: Loss = 0.266091
  Batch 10/29: Loss = 0.242699
  Batch 20/29: Loss = 0.293711

Epoch 174/200 | Loss: 0.260143 | LR: 0.000017 | Time: 1.0s

  ✓ Saved best model (loss=0.260143)
  Batch 0/29: Loss = 0.254205
  Batch 10/29: Loss = 0.248296
  Batch 20/29: Loss = 0.243534

Epoch 175/200 | Loss: 0.269052 | LR: 0.000016 | Time: 1.0s

  Batch 0/29: Loss = 0.257040
  Batch 10/29: Loss = 0.220495
  Batch 20/29: Loss = 0.273412

Epoch 176/200 | Loss: 0.269802 | LR: 0.000015 | Time: 1.1s

  Batch 0/29: Loss = 0.309676
  Batch 10/29: Loss = 0.331325
  Batch 20/29: Loss = 0.233873

Epoch 177/200 | Loss: 0.268572 | LR: 0.000014 | Time: 1.0s

  Batch 0/29: Loss = 0.320322
  Batch 10/29: Loss = 0.271789
  Batch 20/29: Loss = 0.261955

Epoch 178/200 | Loss: 0.267464 | LR: 0.000012 | Time: 1.1s

  Batch 0/29: Loss = 0.301891
  Batch 10/29: Loss = 0.343192
  Batch 20/29: Loss = 0.295154

Epoch 179/200 | Loss: 0.268238 | LR: 0.000011 | Time: 1.1s

  Batch 0/29: Loss = 0.246169
  Batch 10/29: Loss = 0.282121
  Batch 20/29: Loss = 0.338025

Epoch 180/200 | Loss: 0.267004 | LR: 0.000010 | Time: 1.0s

  Batch 0/29: Loss = 0.309801
  Batch 10/29: Loss = 0.233704
  Batch 20/29: Loss = 0.280768

Epoch 181/200 | Loss: 0.265459 | LR: 0.000009 | Time: 1.1s

  Batch 0/29: Loss = 0.225769
  Batch 10/29: Loss = 0.303630
  Batch 20/29: Loss = 0.298259

Epoch 182/200 | Loss: 0.267303 | LR: 0.000008 | Time: 1.1s

  Batch 0/29: Loss = 0.176847
  Batch 10/29: Loss = 0.267352
  Batch 20/29: Loss = 0.296995

Epoch 183/200 | Loss: 0.261795 | LR: 0.000007 | Time: 1.8s

  Batch 0/29: Loss = 0.296871
  Batch 10/29: Loss = 0.306564
  Batch 20/29: Loss = 0.334046

Epoch 184/200 | Loss: 0.261809 | LR: 0.000007 | Time: 1.1s

  Batch 0/29: Loss = 0.307932
  Batch 10/29: Loss = 0.245414
  Batch 20/29: Loss = 0.219822

Epoch 185/200 | Loss: 0.265035 | LR: 0.000006 | Time: 1.0s

  Batch 0/29: Loss = 0.289554
  Batch 10/29: Loss = 0.279363
  Batch 20/29: Loss = 0.297231

Epoch 186/200 | Loss: 0.263720 | LR: 0.000005 | Time: 1.1s

  Batch 0/29: Loss = 0.259962
  Batch 10/29: Loss = 0.235304
  Batch 20/29: Loss = 0.277311

Epoch 187/200 | Loss: 0.266382 | LR: 0.000004 | Time: 1.1s

  Batch 0/29: Loss = 0.297732
  Batch 10/29: Loss = 0.302006
  Batch 20/29: Loss = 0.258771

Epoch 188/200 | Loss: 0.269346 | LR: 0.000004 | Time: 1.1s

  Batch 0/29: Loss = 0.228909
  Batch 10/29: Loss = 0.309858
  Batch 20/29: Loss = 0.287703

Epoch 189/200 | Loss: 0.265877 | LR: 0.000003 | Time: 0.6s

  Batch 0/29: Loss = 0.204021
  Batch 10/29: Loss = 0.249118
  Batch 20/29: Loss = 0.283497

Epoch 190/200 | Loss: 0.263765 | LR: 0.000003 | Time: 1.1s

  Batch 0/29: Loss = 0.218134
  Batch 10/29: Loss = 0.199472
  Batch 20/29: Loss = 0.222800

Epoch 191/200 | Loss: 0.264036 | LR: 0.000002 | Time: 0.6s

  Batch 0/29: Loss = 0.206592
  Batch 10/29: Loss = 0.293750
  Batch 20/29: Loss = 0.253150

Epoch 192/200 | Loss: 0.267531 | LR: 0.000002 | Time: 1.0s

  Batch 0/29: Loss = 0.251294
  Batch 10/29: Loss = 0.304636
  Batch 20/29: Loss = 0.279158

Epoch 193/200 | Loss: 0.260929 | LR: 0.000001 | Time: 0.8s

  Batch 0/29: Loss = 0.260141
  Batch 10/29: Loss = 0.223150
  Batch 20/29: Loss = 0.281589

Epoch 194/200 | Loss: 0.262518 | LR: 0.000001 | Time: 1.1s

  Batch 0/29: Loss = 0.264877
  Batch 10/29: Loss = 0.255278
  Batch 20/29: Loss = 0.230441

Epoch 195/200 | Loss: 0.264179 | LR: 0.000001 | Time: 1.0s

  Batch 0/29: Loss = 0.263074
  Batch 10/29: Loss = 0.194564
  Batch 20/29: Loss = 0.287169

Epoch 196/200 | Loss: 0.262423 | LR: 0.000000 | Time: 1.0s

  Batch 0/29: Loss = 0.272651
  Batch 10/29: Loss = 0.200691
  Batch 20/29: Loss = 0.279429

Epoch 197/200 | Loss: 0.267105 | LR: 0.000000 | Time: 1.1s

  Batch 0/29: Loss = 0.249041
  Batch 10/29: Loss = 0.262556
  Batch 20/29: Loss = 0.269042

Epoch 198/200 | Loss: 0.260078 | LR: 0.000000 | Time: 1.0s

  ✓ Saved best model (loss=0.260078)
  Batch 0/29: Loss = 0.302637
  Batch 10/29: Loss = 0.240013
  Batch 20/29: Loss = 0.176497

Epoch 199/200 | Loss: 0.268667 | LR: 0.000000 | Time: 0.9s

  Batch 0/29: Loss = 0.178933
  Batch 10/29: Loss = 0.292621
  Batch 20/29: Loss = 0.292031

Epoch 200/200 | Loss: 0.268392 | LR: 0.000000 | Time: 0.8s


============================================================
Training completed!
Best loss: 0.260078
============================================================

4. Evaluating...
/home/menserve/Object-centric-representation/src/train_movi.py:401: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()
/home/menserve/Object-centric-representation/src/train_movi.py:434: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()
Test Loss: 0.746576

5. Visualizing results...
Saved to checkpoints/dinov1_normalized/dino_vits16/movi_result.png
Saved to checkpoints/dinov1_normalized/dino_vits16/training_history.png

✅ Training completed!
